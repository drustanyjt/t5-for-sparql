{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931610c0",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [13]</a>'.</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46bd60ec",
   "metadata": {
    "papermill": {
     "duration": 0.008652,
     "end_time": "2023-06-12T02:33:09.816379",
     "exception": false,
     "start_time": "2023-06-12T02:33:09.807727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Text2SPARQL\n",
    "\n",
    "This is a development workbook for getting the hang of training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a4576c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T02:33:09.837796Z",
     "iopub.status.busy": "2023-06-12T02:33:09.836966Z",
     "iopub.status.idle": "2023-06-12T02:33:09.849992Z",
     "shell.execute_reply": "2023-06-12T02:33:09.849364Z"
    },
    "papermill": {
     "duration": 0.026484,
     "end_time": "2023-06-12T02:33:09.852222",
     "exception": false,
     "start_time": "2023-06-12T02:33:09.825738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0cb8748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T02:33:09.870458Z",
     "iopub.status.busy": "2023-06-12T02:33:09.870136Z",
     "iopub.status.idle": "2023-06-12T02:33:11.007369Z",
     "shell.execute_reply": "2023-06-12T02:33:11.006474Z"
    },
    "papermill": {
     "duration": 1.152984,
     "end_time": "2023-06-12T02:33:11.010500",
     "exception": false,
     "start_time": "2023-06-12T02:33:09.857516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.cuda.is_available() -> bool>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fe29b2f",
   "metadata": {
    "papermill": {
     "duration": 0.01201,
     "end_time": "2023-06-12T02:33:11.028158",
     "exception": false,
     "start_time": "2023-06-12T02:33:11.016148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Bannerjee does some preprocessing of the LCQuAD dataset,\n",
    "I try to replicate that here.\n",
    "\n",
    "First we load some files into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcdb068a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T02:33:11.051850Z",
     "iopub.status.busy": "2023-06-12T02:33:11.051312Z",
     "iopub.status.idle": "2023-06-12T02:33:11.279054Z",
     "shell.execute_reply": "2023-06-12T02:33:11.278231Z"
    },
    "papermill": {
     "duration": 0.247378,
     "end_time": "2023-06-12T02:33:11.281917",
     "exception": false,
     "start_time": "2023-06-12T02:33:11.034539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chandrasekhar limit', 'toluene', 'Olympic victor, stadion']\n",
      "[\"Who is the child of Ranavalona I's husband?\",\n",
      " 'SELECT ?answer WHERE { wd:Q169794 wdt:P26 ?X . ?X wdt:P22 ?answer}']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['video', 'head of government']\n",
      "['(', 'rdfs:label', 'by', 'ask']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from os.path import join\n",
    "from pprint import pprint\n",
    "\n",
    "lcquad2_dir = os.path.join(\"baseline\", \"lcquad2\")\n",
    "\n",
    "# LCQuAD2 entity labels\n",
    "with open(join(lcquad2_dir, \"lcq2_labels.pickle\"), \"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "pprint(list(labels[k] for k in ['q51366', 'q15779', 'q23906217']))\n",
    "\n",
    "# Training Data has exactly the same file size as the official one\n",
    "with open(join(lcquad2_dir, \"train.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "pprint(list(data[1][k] for k in [\"question\", \"sparql_wikidata\"]))\n",
    "\n",
    "# Load the relation labels\n",
    "with open(join(lcquad2_dir, \"relations.json\")) as f:\n",
    "    rel_labels = json.load(f)\n",
    "\n",
    "pprint(list(rel_labels[k] for k in [\"P10\", \"P6\"]))\n",
    "\n",
    "# Load the sparql vocabulary\n",
    "with open(join(lcquad2_dir, \"vocab.txt\")) as f:\n",
    "    vocab = list(map(lambda x: x.strip(), f.readlines()))\n",
    "    vocab.append('null') # not too sure what this is for\n",
    "\n",
    "pprint(vocab[1:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf7d028f",
   "metadata": {
    "papermill": {
     "duration": 0.010236,
     "end_time": "2023-06-12T02:33:11.299331",
     "exception": false,
     "start_time": "2023-06-12T02:33:11.289095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Some labels are missing from the lcq2_labels.pickle,\n",
    "and cause run time errors in the script.\n",
    "We add them back here to avoid this problem\n",
    "(though ideally we should find a better label to entity map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17b52fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T02:33:11.323394Z",
     "iopub.status.busy": "2023-06-12T02:33:11.322856Z",
     "iopub.status.idle": "2023-06-12T02:33:11.330367Z",
     "shell.execute_reply": "2023-06-12T02:33:11.329545Z"
    },
    "papermill": {
     "duration": 0.020994,
     "end_time": "2023-06-12T02:33:11.332080",
     "exception": false,
     "start_time": "2023-06-12T02:33:11.311086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels['quercia']='null'\n",
    "labels['qui']='null'\n",
    "labels['}']='null'\n",
    "labels['p5122'] = 'Ontario public library ID'.lower()\n",
    "labels['p3888']='Boijmans artist ID'\n",
    "labels['p5388']='Bulgarian Antarctic Gazetteer ID'\n",
    "labels['p5151']='Israel Film Fund ID'\n",
    "labels['p3633']='British Museum place ID'\n",
    "labels['p1733']='Steam application ID'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ceaa676",
   "metadata": {
    "papermill": {
     "duration": 0.01181,
     "end_time": "2023-06-12T02:33:11.347570",
     "exception": false,
     "start_time": "2023-06-12T02:33:11.335760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next we assign vocabularies to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9fa7dbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T02:33:11.423349Z",
     "iopub.status.busy": "2023-06-12T02:33:11.422680Z",
     "iopub.status.idle": "2023-06-12T02:33:11.430692Z",
     "shell.execute_reply": "2023-06-12T02:33:11.429923Z"
    },
    "papermill": {
     "duration": 0.022755,
     "end_time": "2023-06-12T02:33:11.432859",
     "exception": false,
     "start_time": "2023-06-12T02:33:11.410104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<extra_id_0>', '<extra_id_60>', '<extra_id_16>']\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "for i, text in enumerate(vocab):\n",
    "    vocab_dict[text] = f'<extra_id_{i}>'\n",
    "\n",
    "pprint([vocab_dict[k] for k in ['\"', 'null', '?value']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ceaf2579",
   "metadata": {
    "papermill": {
     "duration": 0.011455,
     "end_time": "2023-06-12T02:33:11.450319",
     "exception": false,
     "start_time": "2023-06-12T02:33:11.438864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And adjust some labels to use the null token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828694cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T02:33:11.473396Z",
     "iopub.status.busy": "2023-06-12T02:33:11.472842Z",
     "iopub.status.idle": "2023-06-12T02:33:11.486439Z",
     "shell.execute_reply": "2023-06-12T02:33:11.485292Z"
    },
    "papermill": {
     "duration": 0.028321,
     "end_time": "2023-06-12T02:33:11.489101",
     "exception": false,
     "start_time": "2023-06-12T02:33:11.460780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in labels:\n",
    "    if labels[k] is None:\n",
    "        labels[k] = vocab_dict['null']\n",
    "        # print(f'{k}: {labels[k]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "941a3cf6",
   "metadata": {
    "papermill": {
     "duration": 0.010921,
     "end_time": "2023-06-12T02:33:11.506161",
     "exception": false,
     "start_time": "2023-06-12T02:33:11.495240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Some Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c884485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T02:33:11.529411Z",
     "iopub.status.busy": "2023-06-12T02:33:11.529047Z",
     "iopub.status.idle": "2023-06-12T02:33:11.535582Z",
     "shell.execute_reply": "2023-06-12T02:33:11.534556Z"
    },
    "papermill": {
     "duration": 0.02116,
     "end_time": "2023-06-12T02:33:11.537873",
     "exception": false,
     "start_time": "2023-06-12T02:33:11.516713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xprint(thing):\n",
    "    pprint(thing)\n",
    "    return thing\n",
    "\n",
    "def compare(x, y=None):\n",
    "\n",
    "    def _compare(z):\n",
    "        pprint(f\"Old: {x}\")\n",
    "        pprint(f\"New: {z}\")\n",
    "    \n",
    "    if not y:\n",
    "        return lambda z : _compare(z)\n",
    "    else:\n",
    "        return lambda : _compare(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36054b46",
   "metadata": {
    "papermill": {
     "duration": 0.011513,
     "end_time": "2023-06-12T02:33:11.555446",
     "exception": false,
     "start_time": "2023-06-12T02:33:11.543933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we reformate the dataset\n",
    "- Note: it seems that Bannerjee replaces training data\n",
    "that has no questions with the Natural Language version.\n",
    "\n",
    "For reference these are the definition of each feature,\n",
    "taken **verbatim** from their [homepage](https://sda.tech/projects/lc-quad-2/)\n",
    "```\n",
    "{\n",
    "     \"uid\": a unique id number\n",
    "     \"sparql_wikidata\": a sparql fro wikidata endpoint\n",
    "     \"sparql_dbpedia18\": a sparql for DBpedia endpoint which has wikidata information\n",
    "     \"NNQT_question\": system generated question,\n",
    "     \"question\": Verbalised question,\n",
    "     \"paraphrased_question\": paraphrased version of the verbalised question,\n",
    "     \"template_id\": id for the template\n",
    "     \"template\": template discription    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef1cd55a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T02:33:11.580377Z",
     "iopub.status.busy": "2023-06-12T02:33:11.579860Z",
     "iopub.status.idle": "2023-06-12T02:33:12.842600Z",
     "shell.execute_reply": "2023-06-12T02:33:12.841372Z"
    },
    "papermill": {
     "duration": 1.277622,
     "end_time": "2023-06-12T02:33:12.845277",
     "exception": false,
     "start_time": "2023-06-12T02:33:11.567655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data_x, data_y = [], []\n",
    "data_x_shuffle = []\n",
    "\n",
    "for i, inst in enumerate(data):\n",
    "    wikisparql = inst['sparql_wikidata']\n",
    "    if inst['question'] is None:\n",
    "        question = inst['NNQT_question']\n",
    "    else:\n",
    "        question = inst['question']\n",
    "    question = question.replace('{', '').replace('}', '')\n",
    "\n",
    "    match_str = r\"\\'(.*?)\\'\"\n",
    "    hashi = {}\n",
    "    # To mask filter literals\n",
    "    if re.search(match_str, wikisparql):\n",
    "        lits=re.findall(match_str,wikisparql)\n",
    "        # print(f\"Old: {wikisparql}\")\n",
    "        for j, lit in enumerate(lits):\n",
    "            idx = j + 1\n",
    "            wikisparql = wikisparql.replace(f\"'{lit.strip()}'\", f\"'###{idx}'\")\n",
    "            hashi[f'###{idx}'] = lit.strip()\n",
    "        # print(f\"New: {wikisparql}\")\n",
    "    \n",
    "    # there is an extra space beacuse of http: and https:\n",
    "    sparql = wikisparql.replace('(',' ( ').replace(')',' ) ') \\\n",
    "    .replace('{',' { '). \\\n",
    "    replace('}',' } ').replace('wd:','wd: ').replace('wdt:','wdt: '). \\\n",
    "    replace(' p:',' p: ').replace(' ps:',' ps: ').replace('pq:','pq: '). \\\n",
    "    replace(',',' , ').replace(\",'\",\", '\").replace(\"'\",\" ' \").replace('.',' . '). \\\n",
    "    replace('=',' = ').replace('  ',' ').lower()\n",
    "    \n",
    "    # print(f\"sparql: {sparql}\")\n",
    "    # select distinct ?obj where { wd: q188920 wdt: p2813 ?obj . ?obj wdt: p31 wd: q1002697 } \n",
    "\n",
    "    _ents = re.findall( r'wd: (?:.*?) ', sparql) # ['wd: q188920 ', 'wd: q1002697 ']\n",
    "    _ents_for_labels = re.findall( r'wd: (.*?) ', sparql) # ['q188920', 'q1002697']\n",
    "    \n",
    "    _rels = re.findall( r'wdt: (?:.*?) ',sparql)\n",
    "    _rels += re.findall( r' p: (?:.*?) ',sparql)\n",
    "    _rels += re.findall( r' ps: (?:.*?) ',sparql)\n",
    "    _rels += re.findall( r'pq: (?:.*?) ',sparql) # ['wdt: p2813 ', 'wdt: p31 ']\n",
    "    # Missing rdfs:label, not sure if that is important\n",
    "    \n",
    "    _rels_for_labels = re.findall( r'wdt: (.*?) ',sparql)\n",
    "    _rels_for_labels += re.findall( r' p: (.*?) ',sparql)\n",
    "    _rels_for_labels += re.findall( r' ps: (.*?) ',sparql)\n",
    "    _rels_for_labels += re.findall( r'pq: (.*?) ',sparql) # ['p2813', 'p31']\n",
    "\n",
    "    # print(_rels)\n",
    "    # print(_rels_for_labels)\n",
    "    for j in range(len(_ents_for_labels)):\n",
    "        # print('Q'+_ents_for_labels[j][1:])\n",
    "        if '}' in _ents[j]: # Entry 12686 is malformed\n",
    "            # pprint(inst)\n",
    "            # pprint(_ents)\n",
    "            _ents[j]=''\n",
    "        _ents[j]=_ents[j]+labels[_ents_for_labels[j]]+' '\n",
    "        # wd: q36970 -> wd: q36970 Jackie Chan\n",
    "\n",
    "    for j in range(len(_rels_for_labels)):\n",
    "        if _rels_for_labels[j].upper() not in rel_labels:\n",
    "            # For some reasons the original preprocess.py didnt convert to upper?\n",
    "            rel_labels['P'+_rels_for_labels[j][1:]]=vocab_dict['null']\n",
    "        _rels[j]=_rels[j]+rel_labels['P'+_rels_for_labels[j][1:]]+' '\n",
    "        # wdt: p26 -> wdt: p26 spouse\n",
    "    # print(_ents)\n",
    "\n",
    "    _ents+=_rels\n",
    "    # random.shuffle(_ents)\n",
    "    # random.shuffle(_rels)\n",
    "\n",
    "    # move to a function\n",
    "    newvars = ['?vr0','?vr1','?vr2','?vr3','?vr4','?vr5']\n",
    "    sparql_split = sparql.split()\n",
    "    variables = set([x for x in sparql_split if x[0] == '?'])\n",
    "    for j, var in enumerate(sorted(variables)):\n",
    "        if var == '?maskvar1': #???\n",
    "            print(sparql)\n",
    "            continue\n",
    "        sparql = sparql.replace(var, newvars[j]) # Normalize var names\n",
    "    \n",
    "    # old = compare(sparql)\n",
    "\n",
    "    split = sparql.split()\n",
    "    \n",
    "    for j, item in enumerate(split):\n",
    "        if item in vocab_dict:\n",
    "            split[j] = vocab_dict[item]\n",
    "    \n",
    "    split = ' '.join(split).strip()\n",
    "    # old(split)\n",
    "\n",
    "    for keys in hashi:\n",
    "        split = split.replace(keys, hashi[keys])\n",
    "    \n",
    "    data_y.append(split)\n",
    "\n",
    "    for rel in _ents:\n",
    "        rel=rel.replace('wd:',vocab_dict['wd:']+' ')\n",
    "        rel=rel.replace('wdt:',vocab_dict['wdt:']+' ')\n",
    "        old = compare(rel)\n",
    "        if 'p:' in rel:\n",
    "            if 'http' in rel:\n",
    "                print(inst) # There are no more http\n",
    "            rel=rel.replace('p:',vocab_dict['p:']+' ')\n",
    "            # old(rel)\n",
    "        rel=rel.replace('ps:',vocab_dict['ps:']+' ')\n",
    "        rel=rel.replace('pq:',vocab_dict['pq:']+' ')\n",
    "        question=question+' '+vocab_dict['[DEF]']+' '+rel\n",
    "    data_x.append(question.strip())\n",
    "\n",
    "assert len(data_x) == len(data_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef8ff1e8",
   "metadata": {
    "papermill": {
     "duration": 0.009458,
     "end_time": "2023-06-12T02:33:12.861967",
     "exception": false,
     "start_time": "2023-06-12T02:33:12.852509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we need to save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522a4cfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T02:33:12.883842Z",
     "iopub.status.busy": "2023-06-12T02:33:12.883227Z",
     "iopub.status.idle": "2023-06-12T02:33:13.623335Z",
     "shell.execute_reply": "2023-06-12T02:33:13.622116Z"
    },
    "papermill": {
     "duration": 0.754396,
     "end_time": "2023-06-12T02:33:13.626766",
     "exception": false,
     "start_time": "2023-06-12T02:33:12.872370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'x': data_x,\n",
    "    'y': data_y,\n",
    "    })\n",
    "\n",
    "save_file = join(lcquad2_dir, 'preprocessed_data.csv')\n",
    "df.to_csv(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "616fcbbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T02:33:13.648472Z",
     "iopub.status.busy": "2023-06-12T02:33:13.648082Z",
     "iopub.status.idle": "2023-06-12T02:33:13.659875Z",
     "shell.execute_reply": "2023-06-12T02:33:13.659163Z"
    },
    "papermill": {
     "duration": 0.025729,
     "end_time": "2023-06-12T02:33:13.662242",
     "exception": false,
     "start_time": "2023-06-12T02:33:13.636513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What periodical literature does Delta Air Line...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_21&gt; &lt;extra_id_39&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is the child of Ranavalona I's husband? &lt;e...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_39&gt; &lt;extra_id_19&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is it true Jeff_Bridges occupation Lane Chandl...</td>\n",
       "      <td>&lt;extra_id_4&gt; &lt;extra_id_19&gt; &lt;extra_id_33&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the pre-requisite of phase matter of G...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_39&gt; &lt;extra_id_19&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which is the operating income for Qantas? &lt;ext...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_21&gt; &lt;extra_id_39&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   x  \\\n",
       "0  What periodical literature does Delta Air Line...   \n",
       "1  Who is the child of Ranavalona I's husband? <e...   \n",
       "2  Is it true Jeff_Bridges occupation Lane Chandl...   \n",
       "3  What is the pre-requisite of phase matter of G...   \n",
       "4  Which is the operating income for Qantas? <ext...   \n",
       "\n",
       "                                                   y  \n",
       "0  <extra_id_6> <extra_id_21> <extra_id_39> <extr...  \n",
       "1  <extra_id_6> <extra_id_39> <extra_id_19> <extr...  \n",
       "2  <extra_id_4> <extra_id_19> <extra_id_33> <extr...  \n",
       "3  <extra_id_6> <extra_id_39> <extra_id_19> <extr...  \n",
       "4  <extra_id_6> <extra_id_21> <extra_id_39> <extr...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc48e8ad",
   "metadata": {
    "papermill": {
     "duration": 0.009887,
     "end_time": "2023-06-12T02:33:13.678164",
     "exception": false,
     "start_time": "2023-06-12T02:33:13.668277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "\n",
    "Now we need to generate a T5 model for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c73f047",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T02:33:13.699772Z",
     "iopub.status.busy": "2023-06-12T02:33:13.699407Z",
     "iopub.status.idle": "2023-06-12T02:33:14.126969Z",
     "shell.execute_reply": "2023-06-12T02:33:14.126025Z"
    },
    "papermill": {
     "duration": 0.444082,
     "end_time": "2023-06-12T02:33:14.130145",
     "exception": false,
     "start_time": "2023-06-12T02:33:13.686063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import transformers\n",
    "# from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")\n",
    "        pprint(self.model.hf_device_map)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        outputs = self.model(\n",
    "            input_ids = input['input_ids'],\n",
    "            labels = input['labels'],\n",
    "            attention_mask = input['attention_mask'],\n",
    "            output_hidden_states = True,\n",
    "            output_attentions = True\n",
    "        )\n",
    "\n",
    "        return outputs.loss\n",
    "\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\") # Device_map splits the load over multiple GPUs, this seems to be quite new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42db36f6",
   "metadata": {
    "papermill": {
     "duration": 0.010755,
     "end_time": "2023-06-12T02:33:14.150883",
     "exception": false,
     "start_time": "2023-06-12T02:33:14.140128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3872710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T02:33:14.174300Z",
     "iopub.status.busy": "2023-06-12T02:33:14.173700Z",
     "iopub.status.idle": "2023-06-12T02:33:14.297045Z",
     "shell.execute_reply": "2023-06-12T02:33:14.296141Z"
    },
    "papermill": {
     "duration": 0.141031,
     "end_time": "2023-06-12T02:33:14.301021",
     "exception": false,
     "start_time": "2023-06-12T02:33:14.159990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class Train:\n",
    "    def __init__(self,data,data_val, model_name):\n",
    "        self.data=data\n",
    "        self.dev_data=data_val\n",
    "\n",
    "        self.tokenizer=T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model=Model(model_name)\n",
    "        # self.model.to(f'cuda:{self.model.device_ids[0]}')  \n",
    "           \n",
    "        # Modify lr?\n",
    "        self.optimizer=optim.AdamW(self.model.parameters(),lr=0.0015)\n",
    "        self.lr_scheduler=transformers. \\\n",
    "        get_polynomial_decay_schedule_with_warmup(self.optimizer, 5000, 30000,power=0.5)\n",
    "\n",
    "        self.iters=60000\n",
    "        self.print_every=100\n",
    "        self.eval_every=8000\n",
    "        # self.num_gpus=1\n",
    "        self.eval_bs=6\n",
    "        self.bs=5\n",
    "        self.back_propogate=10\n",
    "        \n",
    "        self.train()\n",
    "\n",
    "    def generate_batch(self):\n",
    "        output=random.sample(self.data,self.bs)\n",
    "        inp,label=[],[]\n",
    "        for dat in output:\n",
    "            inp.append(dat[0])\n",
    "            label.append(dat[1])\n",
    "\n",
    "        return inp,label\n",
    "\n",
    "    def preprocess_function(self,inputs, targets):\n",
    "        model_inputs=self.tokenizer(inputs, padding=True, \\\n",
    "                        return_tensors='pt',max_length=512, truncation=True)\n",
    "        labels=self.tokenizer(targets,padding=True,max_length=512, truncation=True)\n",
    "\n",
    "        if True:\n",
    "            labels[\"input_ids\"] = [\n",
    "            [(l if l != self.tokenizer.pad_token_id else -100) \\\n",
    "             for l in label] for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "        labels['input_ids']=torch.tensor(labels['input_ids'])\n",
    "        model_inputs[\"labels\"]=labels[\"input_ids\"].to(0)\n",
    "        model_inputs[\"input_ids\"]=model_inputs[\"input_ids\"].to(0)\n",
    "        model_inputs[\"attention_mask\"]=model_inputs[\"attention_mask\"].to(0)\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    def val(self,o):\n",
    "        print('Evaluating ...')\n",
    "        self.model.eval()\n",
    "        acc,bs,i=0,self.eval_bs,0\n",
    "        saver=[]\n",
    "\n",
    "        progress_bar = tqdm.auto.tqdm(range(math.ceil(len(self.dev_data) / bs)))\n",
    "        progress_bar.set_description(f\"Eval {o}\")\n",
    "           \n",
    "        while i<len(self.dev_data):\n",
    "            bs_=min(bs,len(self.dev_data)-i)\n",
    "            i+=bs_\n",
    "            inp,label=[],[]\n",
    "            for j in range(i-bs_,i):\n",
    "                inp.append(self.dev_data[j][0])\n",
    "                label.append(self.dev_data[j][1])\n",
    "            \n",
    "\n",
    "            input=self.preprocess_function(inp,label)\n",
    "\n",
    "            output=self.model.model.generate(input_ids=input['input_ids'],\n",
    "                      num_beams=10,attention_mask=input['attention_mask'], \\\n",
    "                        early_stopping=True, max_length=200,output_hidden_states=True,output_attentions=True)\n",
    "            \n",
    "            out=self.tokenizer.batch_decode(output,skip_special_tokens=False)\n",
    "\n",
    "            for k in range(len(out)):\n",
    "                #print(out[k].replace('<pad>','').replace('</s>','').strip())\n",
    "                a1=out[k].replace('<pad>','').replace('</s>','').replace('<unk>','').replace('<s>','').strip().replace(' ','')\n",
    "                a2=label[k].strip().replace(' ','')\n",
    "                #print(a1, '       ', a2)\n",
    "                saver.append({'input':inp[k],'gold':label[k].strip(),'generated':out[k].replace('<pad>',''). \\\n",
    "                      replace('</s>','').replace('<unk>','').replace('<s>','').strip()})\n",
    "                if a1==a2:\n",
    "                    acc+=1; #print('ttt')\n",
    "\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        file=open('_dev_result'+str(o)+'.json','w')\n",
    "        json.dump(saver,file)\n",
    "        file.close()\n",
    "        return 100*acc/len(self.dev_data)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        scalar=0\n",
    "        for i in range(self.iters):\n",
    "            self.model.train()\n",
    "            inp,label=self.generate_batch()\n",
    "            input=self.preprocess_function(inp,label)\n",
    "            loss=self.model(input)\n",
    "\n",
    "            scalar+=loss.mean().item()\n",
    "            if(i+1)%self.print_every==0:\n",
    "                print('iteration={}, training loss={}'.format(i+1,scalar/self.print_every))\n",
    "                scalar=0\n",
    "            if(i + 1)%self.eval_every==0:\n",
    "                acc=self.val(i+1)\n",
    "                print('validation acc={}'.format(acc))\n",
    "\n",
    "                torch.save(self.model.state_dict(),\n",
    "                       join(lcquad2_dir,'checkpoints','checkpoint_'+str(i+1)+'.pth'))\n",
    "            \n",
    "            loss/=self.back_propogate\n",
    "            loss.mean().backward()\n",
    "            if (i+1)%self.back_propogate:\n",
    "                self.optimizer.step();\n",
    "                self.lr_scheduler.step();\n",
    "                self.optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5497f1d",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1219dcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T02:33:14.322402Z",
     "iopub.status.busy": "2023-06-12T02:33:14.322112Z",
     "iopub.status.idle": "2023-06-12T03:07:05.817733Z",
     "shell.execute_reply": "2023-06-12T03:07:05.816388Z"
    },
    "papermill": {
     "duration": 2031.514204,
     "end_time": "2023-06-12T03:07:05.822501",
     "exception": true,
     "start_time": "2023-06-12T02:33:14.308297",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=100, training loss=11.813318510055542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=200, training loss=8.44846284866333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=300, training loss=5.113059861660004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=400, training loss=3.477501680850983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=500, training loss=2.7797235298156737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=600, training loss=2.493100411891937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=700, training loss=2.2501452660560606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=800, training loss=2.048100073337555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=900, training loss=1.9094197523593903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=1000, training loss=1.6779260671138763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=1100, training loss=1.5564465534687042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=1200, training loss=1.394128408432007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=1300, training loss=1.2887245059013366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=1400, training loss=1.2048748314380646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=1500, training loss=1.1205632644891739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=1600, training loss=1.0330032974481582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=1700, training loss=0.9404845470190049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=1800, training loss=0.9216135931015015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=1900, training loss=0.8201875853538513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=2000, training loss=0.7534273067116737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=2100, training loss=0.6923303878307343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=2200, training loss=0.6171920099854469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=2300, training loss=0.582698864042759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=2400, training loss=0.5155231319367886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=2500, training loss=0.4625528338551521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=2600, training loss=0.5011101287603378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=2700, training loss=0.4435797676444054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=2800, training loss=0.3745379883050919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=2900, training loss=0.3454596146941185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=3000, training loss=0.3202062539756298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=3100, training loss=0.2676612882316112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=3200, training loss=0.2748552058637142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=3300, training loss=0.23582274325191974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=3400, training loss=0.22121315058320762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=3500, training loss=0.18695718806236983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=3600, training loss=0.15762619581073523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=3700, training loss=0.15590337242931127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=3800, training loss=0.20094797998666764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=3900, training loss=0.1595303849875927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=4000, training loss=0.19616647850722074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=4100, training loss=0.1575426685810089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=4200, training loss=0.15601395722478628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=4300, training loss=0.1284921018406749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=4400, training loss=0.16240025093778968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=4500, training loss=0.19690770130604507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=4600, training loss=0.15721602234989404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=4700, training loss=0.14124349031597375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=4800, training loss=0.11302984887734055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=4900, training loss=0.13951074259355664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=5000, training loss=0.12419052638113498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=5100, training loss=0.09701643175445497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=5200, training loss=0.11313457136973738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=5300, training loss=0.1779134514182806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=5400, training loss=0.15604967100545764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=5500, training loss=0.14774515675380825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=5600, training loss=0.11519388839602471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=5700, training loss=0.10371140731498599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=5800, training loss=0.0960153347812593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=5900, training loss=0.10240726809948683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=6000, training loss=0.07364873210899532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=6100, training loss=0.06364788940176368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=6200, training loss=0.06240085673052818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=6300, training loss=0.04914734915830195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=6400, training loss=0.06085878948215395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=6500, training loss=0.04286894359625876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=6600, training loss=0.051652923175133765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=6700, training loss=0.05766490859445184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=6800, training loss=0.07604447688441723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=6900, training loss=0.07043570089619607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=7000, training loss=0.05847912264056504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=7100, training loss=0.051052685058675704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=7200, training loss=0.0347578337485902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=7300, training loss=0.05848886219086125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=7400, training loss=0.08201636651530862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=7500, training loss=0.06489396713674069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=7600, training loss=0.06519550327211618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=7700, training loss=0.06756882157875226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=7800, training loss=0.04086372191319242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=7900, training loss=0.0385817784559913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=8000, training loss=0.04515260442392901\n",
      "Evaluating ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2495744989e4fd2b3ed75295bb22c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.92 GiB total capacity; 1.77 GiB already allocated; 6.50 MiB free; 2.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m total_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m      3\u001b[0m final_data, final_data_dev \u001b[38;5;241m=\u001b[39m data[:total_len\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m], data[total_len\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m:]\n\u001b[0;32m----> 4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_data_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt5-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m, in \u001b[0;36mTrain.__init__\u001b[0;34m(self, data, data_val, model_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mback_propogate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 110\u001b[0m, in \u001b[0;36mTrain.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     scalar\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_every\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 110\u001b[0m     acc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation acc=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(acc))\n\u001b[1;32m    113\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    114\u001b[0m            join(lcquad2_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[12], line 73\u001b[0m, in \u001b[0;36mTrain.val\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m     68\u001b[0m     label\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdev_data[j][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_function(inp,label)\n\u001b[0;32m---> 73\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m          \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(output,skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(out)):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m#print(out[k].replace('<pad>','').replace('</s>','').strip())\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/generation/utils.py:1524\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1523\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/generation/utils.py:2810\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2806\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   2808\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2810\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2811\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2815\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2818\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1716\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1713\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1716\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1731\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1086\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1074\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1075\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     )\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1086\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:693\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 693\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    703\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:600\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    591\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    597\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m ):\n\u001b[1;32m    599\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 600\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    610\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:525\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# get key/value states\u001b[39;00m\n\u001b[1;32m    522\u001b[0m key_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[1;32m    523\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, key_value_states, past_key_value[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    524\u001b[0m )\n\u001b[0;32m--> 525\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# compute scores\u001b[39;00m\n\u001b[1;32m    530\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\n\u001b[1;32m    531\u001b[0m     query_states, key_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    532\u001b[0m )  \u001b[38;5;66;03m# equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:506\u001b[0m, in \u001b[0;36mT5Attention.forward.<locals>.project\u001b[0;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_value_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;66;03m# self-attn\u001b[39;00m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;66;03m# (batch_size, n_heads, key_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m past_key_value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m key_value_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;66;03m# checking that the `sequence_length` of the `past_key_value` is the same as\u001b[39;00m\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;66;03m# the provided `key_value_states` to support prefix tuning\u001b[39;00m\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;66;03m# cross-attn\u001b[39;00m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    512\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m shape(proj_layer(key_value_states))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.92 GiB total capacity; 1.77 GiB already allocated; 6.50 MiB free; 2.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "data = df.values.tolist()\n",
    "total_len = len(data)\n",
    "final_data, final_data_dev = data[:total_len//10], data[total_len//10:]\n",
    "trainer = Train(final_data, final_data_dev, \"t5-small\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2038.414026,
   "end_time": "2023-06-12T03:07:06.965433",
   "environment_variables": {},
   "exception": true,
   "input_path": "text2sparql.ipynb",
   "output_path": "text2sparql.papermill.ipynb",
   "parameters": {},
   "start_time": "2023-06-12T02:33:08.551407",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03aa1598faa34802b23a083820c71561": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "479fc8842d0c407ba3e0e693c6f0787c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "47fdf0b6e780463d96f417df9d3c32bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6debb1726b0a45c5942a545416f690c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a08dcadde1d94c6e87def56ee9929d9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c6ad1be5c094464e8682a48c29395efb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_47fdf0b6e780463d96f417df9d3c32bb",
       "max": 3627.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a08dcadde1d94c6e87def56ee9929d9e",
       "tabbable": null,
       "tooltip": null,
       "value": 1409.0
      }
     },
     "dce71e7338dd4f52b59f0eb4f322b6e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e2495744989e4fd2b3ed75295bb22c09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ec9041a96f194c2997ac48d89e31e37f",
        "IPY_MODEL_c6ad1be5c094464e8682a48c29395efb",
        "IPY_MODEL_f38e4223fa6f408c8883eed645ca6a46"
       ],
       "layout": "IPY_MODEL_e2638017a4a343b6a6bfa05f7b877a33",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e2638017a4a343b6a6bfa05f7b877a33": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec9041a96f194c2997ac48d89e31e37f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_03aa1598faa34802b23a083820c71561",
       "placeholder": "",
       "style": "IPY_MODEL_dce71e7338dd4f52b59f0eb4f322b6e2",
       "tabbable": null,
       "tooltip": null,
       "value": "Eval 8000:  39%"
      }
     },
     "f38e4223fa6f408c8883eed645ca6a46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_479fc8842d0c407ba3e0e693c6f0787c",
       "placeholder": "",
       "style": "IPY_MODEL_6debb1726b0a45c5942a545416f690c6",
       "tabbable": null,
       "tooltip": null,
       "value": " 1409/3627 [21:40&lt;33:11,  1.11it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}