{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import json\n",
    "import tqdm\n",
    "import transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIIBLE_DEVICES\"] = \"0,1\"\n",
    "NUM_EPOCHS = 50\n",
    "EXPERIMENT_NAME = \"t5-small_falcon2-default_annotation-default\"\n",
    "EXPERIMENT_DIR = Path('experiments')\n",
    "MODEL_ARTIFACTS = EXPERIMENT_DIR / EXPERIMENT_DIR\n",
    "WEIGHTS_DIR = MODEL_ARTIFACTS / 'weights'\n",
    "VALS_DIR = MODEL_ARTIFACTS / 'validations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make appropriate directoreis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VALS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"t5-small\"\n",
    "tokenizer_path = \"t5-small\"\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path).to(\"cuda\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_path).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataset maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df):\n",
    "  # ratios from Bannerjee\n",
    "  train = 0.7\n",
    "  dev = 0.1\n",
    "  test = 0.2\n",
    "  assert train + dev + test == 1.0\n",
    "  data_len = len(df)\n",
    "  train_set = Dataset.from_pandas(df[:round(data_len * train)])\n",
    "  dev_set = Dataset.from_pandas(df[round(data_len * train):round(data_len* (train + dev))])\n",
    "  test_set = Dataset.from_pandas(df[round(data_len * (train + dev)):])\n",
    "  \n",
    "  dataset = DatasetDict()\n",
    "  dataset['train'] = train_set\n",
    "  dataset['dev'] = dev_set\n",
    "  dataset['test'] = test_set\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataset tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(dataset, column):\n",
    "  model_inputs = tokenizer(dataset[column], padding=True, return_tensors=\"pt\")\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(val_dataloader, val_path = None):\n",
    "  model.eval()\n",
    "  eval_dict = []\n",
    "  for val_batch in val_dataloader:\n",
    "    batch = {}\n",
    "    for k,v in val_batch.items():\n",
    "      if k in set(\"input_ids\", \"labels\", \"attention_mask\"):\n",
    "        batch[k] = v.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      outputs = model(**batch)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    for i, pred in enumerate(tokenizer.batch_decode(predictions)):\n",
    "      eval_dict.append({\n",
    "        \"Utte\": val_batch['utterance'][i],\n",
    "        \"Anno\": val_batch['annotated'][i],\n",
    "        \"Gold\": val_batch['gold'][i],\n",
    "        \"Gene\": pred, # THIS NEEDS TO BE UNMASKED\n",
    "      })\n",
    "  \n",
    "  if val_path:\n",
    "    with open(val_path, \"w\") as f:\n",
    "      json.dump(eval_dict, f, indent=2)\n",
    "  \n",
    "  model.train()\n",
    "  return eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(df):\n",
    "\n",
    "  assert 'utterance' in df.columns\n",
    "  assert 'annotated' in df.columns\n",
    "  assert 'gold' in df.columns\n",
    "\n",
    "  dataset = split_dataframe(df)\n",
    "  tokenized_dataset = dataset \\\n",
    "    .map(lambda x: tokenize_data(x, 'gold'), batched=True) \\\n",
    "    .rename_column('input_ids', 'labels') \\\n",
    "    .map(lambda x: tokenize_data(x, 'annotated'), batched=True)\n",
    "  \n",
    "  train_dataset = tokenize_data[\"train\"]\n",
    "  dev_dataset = tokenize_data[\"dev\"]\n",
    "  test_dataset = tokenize_data[\"test\"]\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size = 2)\n",
    "  dev_dataloader = DataLoader(dev_dataset, batch_size = 2)\n",
    "\n",
    "  scalar = 0\n",
    "  i = 0\n",
    "\n",
    "  optimizer = optim.AdamW(model.parameters(), lr = 0.0015)\n",
    "  lr_scheduler=transformers. \\\n",
    "    get_polynomial_decay_schedule_with_warmup(optimizer, 5000, 30000, power=0.5)\n",
    "  for epoch in range(num_epochs):\n",
    "    i = 0\n",
    "    iters = len(train_dataloader)\n",
    "    for batch in train_dataloader:\n",
    "      newbatch = {}\n",
    "      for k,v in batch.items():\n",
    "        if k not in [\"label\", \"input\"]:\n",
    "          newbatch[k] = v.to(\"cuda\")\n",
    "      \n",
    "      batch = newbatch\n",
    "\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      scalar += loss.mean().item()\n",
    "\n",
    "      if (i + 1) % 100 == 0:\n",
    "        print(f'iteration = {i + 1}/{iters}, training loss={scalar/100}')\n",
    "        scalar = 0\n",
    "\n",
    "      loss /= 10 \n",
    "      loss.mean().backward()\n",
    "      if (i+1) % 10:\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "      \n",
    "      i += 1\n",
    "    \n",
    "    val(val_dataloader, VALS_DIR / f\"val_{epoch}.json\")\n",
    "\n",
    "    torch.save(model.state_dict(),\n",
    "      WEIGHTS_DIR / f\"cp_{epoch}.pth\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
