{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text2SPARQL\n",
    "\n",
    "This is a development workbook for getting the hang of training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.cuda.is_available() -> bool>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Bannerjee does some preprocessing of the LCQuAD dataset,\n",
    "I try to replicate that here.\n",
    "\n",
    "First we load some files into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chandrasekhar limit', 'toluene', 'Olympic victor, stadion']\n",
      "[\"Who is the child of Ranavalona I's husband?\",\n",
      " 'SELECT ?answer WHERE { wd:Q169794 wdt:P26 ?X . ?X wdt:P22 ?answer}']\n",
      "['video', 'head of government']\n",
      "['(', 'rdfs:label', 'by', 'ask']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from os.path import join\n",
    "from pprint import pprint\n",
    "\n",
    "lcquad2_dir = os.path.join(\"baseline\", \"lcquad2\")\n",
    "\n",
    "# LCQuAD2 entity labels\n",
    "with open(join(lcquad2_dir, \"lcq2_labels.pickle\"), \"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "pprint(list(labels[k] for k in ['q51366', 'q15779', 'q23906217']))\n",
    "\n",
    "# Training Data has exactly the same file size as the official one\n",
    "with open(join(lcquad2_dir, \"train.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "pprint(list(data[1][k] for k in [\"question\", \"sparql_wikidata\"]))\n",
    "\n",
    "# Load the relation labels\n",
    "with open(join(lcquad2_dir, \"relations.json\")) as f:\n",
    "    rel_labels = json.load(f)\n",
    "\n",
    "pprint(list(rel_labels[k] for k in [\"P10\", \"P6\"]))\n",
    "\n",
    "# Load the sparql vocabulary\n",
    "with open(join(lcquad2_dir, \"vocab.txt\")) as f:\n",
    "    vocab = list(map(lambda x: x.strip(), f.readlines()))\n",
    "    vocab.append('null') # not too sure what this is for\n",
    "\n",
    "pprint(vocab[1:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some labels are missing from the lcq2_labels.pickle,\n",
    "and cause run time errors in the script.\n",
    "We add them back here to avoid this problem\n",
    "(though ideally we should find a better label to entity map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['quercia']='null'\n",
    "labels['qui']='null'\n",
    "labels['}']='null'\n",
    "labels['p5122'] = 'Ontario public library ID'.lower()\n",
    "labels['p3888']='Boijmans artist ID'\n",
    "labels['p5388']='Bulgarian Antarctic Gazetteer ID'\n",
    "labels['p5151']='Israel Film Fund ID'\n",
    "labels['p3633']='British Museum place ID'\n",
    "labels['p1733']='Steam application ID'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we assign vocabularies to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<extra_id_0>', '<extra_id_60>', '<extra_id_16>']\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "for i, text in enumerate(vocab):\n",
    "    vocab_dict[text] = f'<extra_id_{i}>'\n",
    "\n",
    "pprint([vocab_dict[k] for k in ['\"', 'null', '?value']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And adjust some labels to use the null token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in labels:\n",
    "    if labels[k] is None:\n",
    "        labels[k] = vocab_dict['null']\n",
    "        # print(f'{k}: {labels[k]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xprint(thing):\n",
    "    pprint(thing)\n",
    "    return thing\n",
    "\n",
    "def compare(x, y=None):\n",
    "\n",
    "    def _compare(z):\n",
    "        pprint(f\"Old: {x}\")\n",
    "        pprint(f\"New: {z}\")\n",
    "    \n",
    "    if not y:\n",
    "        return lambda z : _compare(z)\n",
    "    else:\n",
    "        return lambda : _compare(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reformate the dataset\n",
    "- Note: it seems that Bannerjee replaces training data\n",
    "that has no questions with the Natural Language version.\n",
    "\n",
    "For reference these are the definition of each feature,\n",
    "taken **verbatim** from their [homepage](https://sda.tech/projects/lc-quad-2/)\n",
    "```\n",
    "{\n",
    "     \"uid\": a unique id number\n",
    "     \"sparql_wikidata\": a sparql fro wikidata endpoint\n",
    "     \"sparql_dbpedia18\": a sparql for DBpedia endpoint which has wikidata information\n",
    "     \"NNQT_question\": system generated question,\n",
    "     \"question\": Verbalised question,\n",
    "     \"paraphrased_question\": paraphrased version of the verbalised question,\n",
    "     \"template_id\": id for the template\n",
    "     \"template\": template discription    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data_x, data_y = [], []\n",
    "data_x_shuffle = []\n",
    "\n",
    "for i, inst in enumerate(data):\n",
    "    wikisparql = inst['sparql_wikidata']\n",
    "    if inst['question'] is None:\n",
    "        question = inst['NNQT_question']\n",
    "    else:\n",
    "        question = inst['question']\n",
    "    question = question.replace('{', '').replace('}', '')\n",
    "\n",
    "    match_str = r\"\\'(.*?)\\'\"\n",
    "    hashi = {}\n",
    "    # To mask filter literals\n",
    "    if re.search(match_str, wikisparql):\n",
    "        lits=re.findall(match_str,wikisparql)\n",
    "        # print(f\"Old: {wikisparql}\")\n",
    "        for j, lit in enumerate(lits):\n",
    "            idx = j + 1\n",
    "            wikisparql = wikisparql.replace(f\"'{lit.strip()}'\", f\"'###{idx}'\")\n",
    "            hashi[f'###{idx}'] = lit.strip()\n",
    "        # print(f\"New: {wikisparql}\")\n",
    "    \n",
    "    # there is an extra space beacuse of http: and https:\n",
    "    sparql = wikisparql.replace('(',' ( ').replace(')',' ) ') \\\n",
    "    .replace('{',' { '). \\\n",
    "    replace('}',' } ').replace('wd:','wd: ').replace('wdt:','wdt: '). \\\n",
    "    replace(' p:',' p: ').replace(' ps:',' ps: ').replace('pq:','pq: '). \\\n",
    "    replace(',',' , ').replace(\",'\",\", '\").replace(\"'\",\" ' \").replace('.',' . '). \\\n",
    "    replace('=',' = ').replace('  ',' ').lower()\n",
    "    \n",
    "    # print(f\"sparql: {sparql}\")\n",
    "    # select distinct ?obj where { wd: q188920 wdt: p2813 ?obj . ?obj wdt: p31 wd: q1002697 } \n",
    "\n",
    "    _ents = re.findall( r'wd: (?:.*?) ', sparql) # ['wd: q188920 ', 'wd: q1002697 ']\n",
    "    _ents_for_labels = re.findall( r'wd: (.*?) ', sparql) # ['q188920', 'q1002697']\n",
    "    \n",
    "    _rels = re.findall( r'wdt: (?:.*?) ',sparql)\n",
    "    _rels += re.findall( r' p: (?:.*?) ',sparql)\n",
    "    _rels += re.findall( r' ps: (?:.*?) ',sparql)\n",
    "    _rels += re.findall( r'pq: (?:.*?) ',sparql) # ['wdt: p2813 ', 'wdt: p31 ']\n",
    "    # Missing rdfs:label, not sure if that is important\n",
    "    \n",
    "    _rels_for_labels = re.findall( r'wdt: (.*?) ',sparql)\n",
    "    _rels_for_labels += re.findall( r' p: (.*?) ',sparql)\n",
    "    _rels_for_labels += re.findall( r' ps: (.*?) ',sparql)\n",
    "    _rels_for_labels += re.findall( r'pq: (.*?) ',sparql) # ['p2813', 'p31']\n",
    "\n",
    "    # print(_rels)\n",
    "    # print(_rels_for_labels)\n",
    "    for j in range(len(_ents_for_labels)):\n",
    "        # print('Q'+_ents_for_labels[j][1:])\n",
    "        if '}' in _ents[j]: # Entry 12686 is malformed\n",
    "            # pprint(inst)\n",
    "            # pprint(_ents)\n",
    "            _ents[j]=''\n",
    "        _ents[j]=_ents[j]+labels[_ents_for_labels[j]]+' '\n",
    "        # wd: q36970 -> wd: q36970 Jackie Chan\n",
    "\n",
    "    for j in range(len(_rels_for_labels)):\n",
    "        if _rels_for_labels[j].upper() not in rel_labels:\n",
    "            # For some reasons the original preprocess.py didnt convert to upper?\n",
    "            rel_labels['P'+_rels_for_labels[j][1:]]=vocab_dict['null']\n",
    "        _rels[j]=_rels[j]+rel_labels['P'+_rels_for_labels[j][1:]]+' '\n",
    "        # wdt: p26 -> wdt: p26 spouse\n",
    "    # print(_ents)\n",
    "\n",
    "    _ents+=_rels\n",
    "    # random.shuffle(_ents)\n",
    "    # random.shuffle(_rels)\n",
    "\n",
    "    # move to a function\n",
    "    newvars = ['?vr0','?vr1','?vr2','?vr3','?vr4','?vr5']\n",
    "    sparql_split = sparql.split()\n",
    "    variables = set([x for x in sparql_split if x[0] == '?'])\n",
    "    for j, var in enumerate(sorted(variables)):\n",
    "        if var == '?maskvar1': #???\n",
    "            print(sparql)\n",
    "            continue\n",
    "        sparql = sparql.replace(var, newvars[j]) # Normalize var names\n",
    "    \n",
    "    # old = compare(sparql)\n",
    "\n",
    "    split = sparql.split()\n",
    "    \n",
    "    for j, item in enumerate(split):\n",
    "        if item in vocab_dict:\n",
    "            split[j] = vocab_dict[item]\n",
    "    \n",
    "    split = ' '.join(split).strip()\n",
    "    # old(split)\n",
    "\n",
    "    for keys in hashi:\n",
    "        split = split.replace(keys, hashi[keys])\n",
    "    \n",
    "    data_y.append(split)\n",
    "\n",
    "    for rel in _ents:\n",
    "        rel=rel.replace('wd:',vocab_dict['wd:']+' ')\n",
    "        rel=rel.replace('wdt:',vocab_dict['wdt:']+' ')\n",
    "        old = compare(rel)\n",
    "        if 'p:' in rel:\n",
    "            if 'http' in rel:\n",
    "                print(inst) # There are no more http\n",
    "            rel=rel.replace('p:',vocab_dict['p:']+' ')\n",
    "            # old(rel)\n",
    "        rel=rel.replace('ps:',vocab_dict['ps:']+' ')\n",
    "        rel=rel.replace('pq:',vocab_dict['pq:']+' ')\n",
    "        question=question+' '+vocab_dict['[DEF]']+' '+rel\n",
    "    data_x.append(question.strip())\n",
    "\n",
    "assert len(data_x) == len(data_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'x': data_x,\n",
    "    'y': data_y,\n",
    "    })\n",
    "\n",
    "save_file = join(lcquad2_dir, 'preprocessed_data.csv')\n",
    "df.to_csv(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What periodical literature does Delta Air Line...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_21&gt; &lt;extra_id_39&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is the child of Ranavalona I's husband? &lt;e...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_39&gt; &lt;extra_id_19&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is it true Jeff_Bridges occupation Lane Chandl...</td>\n",
       "      <td>&lt;extra_id_4&gt; &lt;extra_id_19&gt; &lt;extra_id_33&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the pre-requisite of phase matter of G...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_39&gt; &lt;extra_id_19&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which is the operating income for Qantas? &lt;ext...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_21&gt; &lt;extra_id_39&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   x  \\\n",
       "0  What periodical literature does Delta Air Line...   \n",
       "1  Who is the child of Ranavalona I's husband? <e...   \n",
       "2  Is it true Jeff_Bridges occupation Lane Chandl...   \n",
       "3  What is the pre-requisite of phase matter of G...   \n",
       "4  Which is the operating income for Qantas? <ext...   \n",
       "\n",
       "                                                   y  \n",
       "0  <extra_id_6> <extra_id_21> <extra_id_39> <extr...  \n",
       "1  <extra_id_6> <extra_id_39> <extra_id_19> <extr...  \n",
       "2  <extra_id_4> <extra_id_19> <extra_id_33> <extr...  \n",
       "3  <extra_id_6> <extra_id_39> <extra_id_19> <extr...  \n",
       "4  <extra_id_6> <extra_id_21> <extra_id_39> <extr...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Now we need to generate a T5 model for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import transformers\n",
    "# from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")\n",
    "        pprint(self.model.hf_device_map)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        outputs = self.model(\n",
    "            input_ids = input['input_ids'],\n",
    "            labels = input['labels'],\n",
    "            attention_mask = input['attention_mask'],\n",
    "            output_hidden_states = True,\n",
    "            output_attentions = True\n",
    "        )\n",
    "\n",
    "        return outputs.loss\n",
    "\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\") # Device_map splits the load over multiple GPUs, this seems to be quite new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class Train:\n",
    "    def __init__(self,data,data_val, model_name):\n",
    "        self.data=data\n",
    "        self.dev_data=data_val\n",
    "\n",
    "        self.tokenizer=T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model=Model(model_name)\n",
    "        # self.model.to(f'cuda:{self.model.device_ids[0]}')  \n",
    "           \n",
    "        # Modify lr?\n",
    "        self.optimizer=optim.AdamW(self.model.parameters(),lr=0.0015)\n",
    "        self.lr_scheduler=transformers. \\\n",
    "        get_polynomial_decay_schedule_with_warmup(self.optimizer, 5000, 30000,power=0.5)\n",
    "\n",
    "        self.iters=60000\n",
    "        self.print_every=100\n",
    "        self.eval_every=8000\n",
    "        # self.num_gpus=1\n",
    "        self.eval_bs=6\n",
    "        self.bs=5\n",
    "        self.back_propogate=10\n",
    "        \n",
    "        self.train()\n",
    "\n",
    "    def generate_batch(self):\n",
    "        output=random.sample(self.data,self.bs)\n",
    "        inp,label=[],[]\n",
    "        for dat in output:\n",
    "            inp.append(dat[0])\n",
    "            label.append(dat[1])\n",
    "\n",
    "        return inp,label\n",
    "\n",
    "    def preprocess_function(self,inputs, targets):\n",
    "        model_inputs=self.tokenizer(inputs, padding=True, \\\n",
    "                        return_tensors='pt',max_length=512, truncation=True)\n",
    "        labels=self.tokenizer(targets,padding=True,max_length=512, truncation=True)\n",
    "\n",
    "        if True:\n",
    "            labels[\"input_ids\"] = [\n",
    "            [(l if l != self.tokenizer.pad_token_id else -100) \\\n",
    "             for l in label] for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "        labels['input_ids']=torch.tensor(labels['input_ids'])\n",
    "        model_inputs[\"labels\"]=labels[\"input_ids\"].to(0)\n",
    "        model_inputs[\"input_ids\"]=model_inputs[\"input_ids\"].to(0)\n",
    "        model_inputs[\"attention_mask\"]=model_inputs[\"attention_mask\"].to(0)\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    def val(self,o):\n",
    "        print('Evaluating ...')\n",
    "        self.model.eval()\n",
    "        acc,bs,i=0,self.eval_bs,0\n",
    "        saver=[]\n",
    "\n",
    "        progress_bar = tqdm.auto.tqdm(range(math.ceil(len(self.dev_data) / bs)))\n",
    "        progress_bar.set_description(f\"Eval {o}\")\n",
    "           \n",
    "        while i<len(self.dev_data):\n",
    "            bs_=min(bs,len(self.dev_data)-i)\n",
    "            i+=bs_\n",
    "            inp,label=[],[]\n",
    "            for j in range(i-bs_,i):\n",
    "                inp.append(self.dev_data[j][0])\n",
    "                label.append(self.dev_data[j][1])\n",
    "            \n",
    "\n",
    "            input=self.preprocess_function(inp,label)\n",
    "\n",
    "            output=self.model.model.generate(input_ids=input['input_ids'],\n",
    "                      num_beams=10,attention_mask=input['attention_mask'], \\\n",
    "                        early_stopping=True, max_length=200,output_hidden_states=True,output_attentions=True)\n",
    "            \n",
    "            out=self.tokenizer.batch_decode(output,skip_special_tokens=False)\n",
    "\n",
    "            for k in range(len(out)):\n",
    "                #print(out[k].replace('<pad>','').replace('</s>','').strip())\n",
    "                a1=out[k].replace('<pad>','').replace('</s>','').replace('<unk>','').replace('<s>','').strip().replace(' ','')\n",
    "                a2=label[k].strip().replace(' ','')\n",
    "                #print(a1, '       ', a2)\n",
    "                saver.append({'input':inp[k],'gold':label[k].strip(),'generated':out[k].replace('<pad>',''). \\\n",
    "                      replace('</s>','').replace('<unk>','').replace('<s>','').strip()})\n",
    "                if a1==a2:\n",
    "                    acc+=1; #print('ttt')\n",
    "\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        file=open('_dev_result'+str(o)+'.json','w')\n",
    "        json.dump(saver,file)\n",
    "        file.close()\n",
    "        return 100*acc/len(self.dev_data)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        scalar=0\n",
    "        for i in range(self.iters):\n",
    "            self.model.train()\n",
    "            inp,label=self.generate_batch()\n",
    "            input=self.preprocess_function(inp,label)\n",
    "            loss=self.model(input)\n",
    "\n",
    "            scalar+=loss.mean().item()\n",
    "            if(i+1)%self.print_every==0:\n",
    "                print('iteration={}, training loss={}'.format(i+1,scalar/self.print_every))\n",
    "                scalar=0\n",
    "            if(i + 1)%self.eval_every==0:\n",
    "                acc=self.val(i+1)\n",
    "                print('validation acc={}'.format(acc))\n",
    "\n",
    "                torch.save(self.model.state_dict(),\n",
    "                       join(lcquad2_dir,'checkpoints','checkpoint_'+str(i+1)+'.pth'))\n",
    "            \n",
    "            loss/=self.back_propogate\n",
    "            loss.mean().backward()\n",
    "            if (i+1)%self.back_propogate:\n",
    "                self.optimizer.step();\n",
    "                self.lr_scheduler.step();\n",
    "                self.optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n",
      "iteration=100, training loss=11.939642734527588\n",
      "iteration=200, training loss=8.600038080215453\n",
      "iteration=300, training loss=5.145956976413727\n",
      "iteration=400, training loss=3.476008038520813\n",
      "iteration=500, training loss=2.811227233409882\n",
      "iteration=600, training loss=2.4777195596694948\n",
      "iteration=700, training loss=2.2361014807224273\n",
      "iteration=800, training loss=2.0416726994514467\n",
      "iteration=900, training loss=1.8513238942623138\n",
      "iteration=1000, training loss=1.722085566520691\n",
      "iteration=1100, training loss=1.5304810416698456\n",
      "iteration=1200, training loss=1.4670107853412628\n",
      "iteration=1300, training loss=1.2645553135871888\n",
      "iteration=1400, training loss=1.1866125708818436\n",
      "iteration=1500, training loss=1.1076714342832565\n",
      "iteration=1600, training loss=1.0168359881639482\n",
      "iteration=1700, training loss=0.9323363494873047\n",
      "iteration=1800, training loss=0.9074511224031448\n",
      "iteration=1900, training loss=0.8476069250702858\n",
      "iteration=2000, training loss=0.7232895228266716\n",
      "iteration=2100, training loss=0.6554145020246506\n",
      "iteration=2200, training loss=0.6034474113583564\n",
      "iteration=2300, training loss=0.5583013588190079\n",
      "iteration=2400, training loss=0.5048671373724938\n",
      "iteration=2500, training loss=0.45988100081682204\n",
      "iteration=2600, training loss=0.4213023373484612\n",
      "iteration=2700, training loss=0.38817705169320105\n",
      "iteration=2800, training loss=0.3842782028019428\n",
      "iteration=2900, training loss=0.3489260904490948\n",
      "iteration=3000, training loss=0.27251614212989805\n",
      "iteration=3100, training loss=0.300100007802248\n",
      "iteration=3200, training loss=0.2339954801276326\n",
      "iteration=3300, training loss=0.21575601443648337\n",
      "iteration=3400, training loss=0.21498520731925963\n",
      "iteration=3500, training loss=0.20628118343651294\n",
      "iteration=3600, training loss=0.21420277796685697\n",
      "iteration=3700, training loss=0.2033358521386981\n",
      "iteration=3800, training loss=0.17154471458867193\n",
      "iteration=3900, training loss=0.1356072399765253\n",
      "iteration=4000, training loss=0.1474618712067604\n",
      "iteration=4100, training loss=0.1355784321948886\n",
      "iteration=4200, training loss=0.11553884268738329\n",
      "iteration=4300, training loss=0.15281898828223348\n",
      "iteration=4400, training loss=0.13362571788951755\n",
      "iteration=4500, training loss=0.1270774617046118\n",
      "iteration=4600, training loss=0.137951885741204\n",
      "iteration=4700, training loss=0.11861418621614575\n",
      "iteration=4800, training loss=0.17017883311957122\n",
      "iteration=4900, training loss=0.11401820240542293\n",
      "iteration=5000, training loss=0.15680807020515203\n",
      "iteration=5100, training loss=0.11423950500786305\n",
      "iteration=5200, training loss=0.09236156281083822\n",
      "iteration=5300, training loss=0.10182656862773001\n",
      "iteration=5400, training loss=0.12003109416924417\n",
      "iteration=5500, training loss=0.11801349844783544\n",
      "iteration=5600, training loss=0.0806461251154542\n",
      "iteration=5700, training loss=0.08019187651574612\n",
      "iteration=5800, training loss=0.07004342634230852\n",
      "iteration=5900, training loss=0.06451452882960439\n",
      "iteration=6000, training loss=0.04898981393314898\n",
      "iteration=6100, training loss=0.05375219105742872\n",
      "iteration=6200, training loss=0.07126180204562843\n",
      "iteration=6300, training loss=0.061108779646456245\n",
      "iteration=6400, training loss=0.0701530494261533\n",
      "iteration=6500, training loss=0.053723823872860524\n",
      "iteration=6600, training loss=0.0561533018713817\n",
      "iteration=6700, training loss=0.05617837078403681\n",
      "iteration=6800, training loss=0.06684086326975375\n",
      "iteration=6900, training loss=0.04476489423308522\n",
      "iteration=7000, training loss=0.05639752067159862\n",
      "iteration=7100, training loss=0.06089795705396682\n",
      "iteration=7200, training loss=0.05688431679038331\n",
      "iteration=7300, training loss=0.061065870618913325\n",
      "iteration=7400, training loss=0.06197559647727758\n",
      "iteration=7500, training loss=0.04306100183865055\n",
      "iteration=7600, training loss=0.07786648204550146\n",
      "iteration=7700, training loss=0.05377506615128368\n",
      "iteration=7800, training loss=0.054337073103524744\n",
      "iteration=7900, training loss=0.06140145662939176\n",
      "iteration=8000, training loss=0.06796134749893099\n",
      "Evaluating ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807294a204564f469d663d35dbe79cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21762 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc=71.18371473210183\n",
      "iteration=8100, training loss=0.06577775314450264\n",
      "iteration=8200, training loss=0.0561351946555078\n",
      "iteration=8300, training loss=0.04193871220806614\n",
      "iteration=8400, training loss=0.04095956897130236\n",
      "iteration=8500, training loss=0.042490445675794034\n",
      "iteration=8600, training loss=0.033869066435145215\n",
      "iteration=8700, training loss=0.03138192718848586\n",
      "iteration=8800, training loss=0.038385037113912404\n",
      "iteration=8900, training loss=0.036568861317355185\n",
      "iteration=9000, training loss=0.027216022023931145\n",
      "iteration=9100, training loss=0.031207118006423114\n",
      "iteration=9200, training loss=0.052538233010564\n",
      "iteration=9300, training loss=0.03160926111624576\n",
      "iteration=9400, training loss=0.05018111715326086\n",
      "iteration=9500, training loss=0.06389687180519105\n",
      "iteration=9600, training loss=0.08089493946405128\n",
      "iteration=9700, training loss=0.04970502816373482\n",
      "iteration=9800, training loss=0.034213382950983945\n",
      "iteration=9900, training loss=0.0330006261379458\n",
      "iteration=10000, training loss=0.04754122112877667\n",
      "iteration=10100, training loss=0.04312144945142791\n",
      "iteration=10200, training loss=0.035149810911389065\n",
      "iteration=10300, training loss=0.023870758331031538\n",
      "iteration=10400, training loss=0.02990671220351942\n",
      "iteration=10500, training loss=0.022708147474913856\n",
      "iteration=10600, training loss=0.02274362950527575\n",
      "iteration=10700, training loss=0.03272588796855416\n",
      "iteration=10800, training loss=0.02662666260031983\n",
      "iteration=10900, training loss=0.030993414298864083\n",
      "iteration=11000, training loss=0.027381022123154252\n",
      "iteration=11100, training loss=0.03353059409186244\n",
      "iteration=11200, training loss=0.025280080317752435\n",
      "iteration=11300, training loss=0.05262597938766703\n",
      "iteration=11400, training loss=0.04094491731375456\n",
      "iteration=11500, training loss=0.03184786580386571\n",
      "iteration=11600, training loss=0.03430825365183409\n",
      "iteration=11700, training loss=0.04006953595904633\n",
      "iteration=11800, training loss=0.042335308499168604\n",
      "iteration=11900, training loss=0.03656316518667154\n",
      "iteration=12000, training loss=0.01968397465592716\n",
      "iteration=12100, training loss=0.020541820803191512\n",
      "iteration=12200, training loss=0.03445295866462402\n",
      "iteration=12300, training loss=0.024583585398504512\n",
      "iteration=12400, training loss=0.022681300809490493\n",
      "iteration=12500, training loss=0.017301947101368568\n",
      "iteration=12600, training loss=0.017286388406937477\n",
      "iteration=12700, training loss=0.012953218636102975\n",
      "iteration=12800, training loss=0.011764221649500542\n",
      "iteration=12900, training loss=0.010568994236527941\n",
      "iteration=13000, training loss=0.013881722716032528\n",
      "iteration=13100, training loss=0.019716544229595456\n",
      "iteration=13200, training loss=0.013203028734424152\n",
      "iteration=13300, training loss=0.01936511861043982\n",
      "iteration=13400, training loss=0.037692636282299644\n",
      "iteration=13500, training loss=0.042506785230943936\n",
      "iteration=13600, training loss=0.025460769542260096\n",
      "iteration=13700, training loss=0.026234005172154864\n",
      "iteration=13800, training loss=0.03321284279169049\n",
      "iteration=13900, training loss=0.03531354335718788\n",
      "iteration=14000, training loss=0.024552881563431585\n",
      "iteration=14100, training loss=0.014894599093822762\n",
      "iteration=14200, training loss=0.019699855949147603\n",
      "iteration=14300, training loss=0.014171749706729315\n",
      "iteration=14400, training loss=0.01747011254832614\n",
      "iteration=14500, training loss=0.017428771324921398\n",
      "iteration=14600, training loss=0.0313138850289397\n",
      "iteration=14700, training loss=0.025643318445654584\n",
      "iteration=14800, training loss=0.02435747218260076\n",
      "iteration=14900, training loss=0.020645817416952924\n",
      "iteration=15000, training loss=0.01580134230258409\n",
      "iteration=15100, training loss=0.01764856224152027\n",
      "iteration=15200, training loss=0.01743066941096913\n",
      "iteration=15300, training loss=0.019644275244208986\n",
      "iteration=15400, training loss=0.010661145583726465\n",
      "iteration=15500, training loss=0.011883039516687859\n",
      "iteration=15600, training loss=0.017704722284688616\n",
      "iteration=15700, training loss=0.013885053811536636\n",
      "iteration=15800, training loss=0.015684885724040212\n",
      "iteration=15900, training loss=0.020874812208930962\n",
      "iteration=16000, training loss=0.021074221982853487\n",
      "Evaluating ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2727d1de2b44d35b7b7ab6b4d6020e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21762 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc=76.88631559599301\n",
      "iteration=16100, training loss=0.010217391563492129\n",
      "iteration=16200, training loss=0.010215325329481858\n",
      "iteration=16300, training loss=0.02285201825608965\n",
      "iteration=16400, training loss=0.01898204331751913\n",
      "iteration=16500, training loss=0.01776347773586167\n",
      "iteration=16600, training loss=0.025091405847924762\n",
      "iteration=16700, training loss=0.016965085968840866\n",
      "iteration=16800, training loss=0.021707160286605357\n",
      "iteration=16900, training loss=0.02241449722845573\n",
      "iteration=17000, training loss=0.010882251707080286\n",
      "iteration=17100, training loss=0.011581272070470732\n",
      "iteration=17200, training loss=0.012952861436642706\n",
      "iteration=17300, training loss=0.014897078222129495\n",
      "iteration=17400, training loss=0.012311741201847326\n",
      "iteration=17500, training loss=0.012153881146805361\n",
      "iteration=17600, training loss=0.012908777084085159\n",
      "iteration=17700, training loss=0.009721626637910959\n",
      "iteration=17800, training loss=0.011843507255543955\n",
      "iteration=17900, training loss=0.00859814964031102\n",
      "iteration=18000, training loss=0.01724114358512452\n",
      "iteration=18100, training loss=0.02446110781427706\n",
      "iteration=18200, training loss=0.023063644568028393\n",
      "iteration=18300, training loss=0.009437282698781928\n",
      "iteration=18400, training loss=0.007825288825115422\n",
      "iteration=18500, training loss=0.007939909227425233\n",
      "iteration=18600, training loss=0.004284497065236792\n",
      "iteration=18700, training loss=0.009189808680530405\n",
      "iteration=18800, training loss=0.0067512679338688035\n",
      "iteration=18900, training loss=0.011795041195437079\n",
      "iteration=19000, training loss=0.009108915019460255\n",
      "iteration=19100, training loss=0.008392737864633092\n",
      "iteration=19200, training loss=0.008672500941611361\n",
      "iteration=19300, training loss=0.009328317216422875\n",
      "iteration=19400, training loss=0.015111517350596841\n",
      "iteration=19500, training loss=0.016101483829843347\n",
      "iteration=19600, training loss=0.02140254924292094\n",
      "iteration=19700, training loss=0.011239305187773425\n",
      "iteration=19800, training loss=0.007181250212452142\n",
      "iteration=19900, training loss=0.005020852979214396\n",
      "iteration=20000, training loss=0.008815983090607915\n",
      "iteration=20100, training loss=0.007729622730985284\n",
      "iteration=20200, training loss=0.012027797485789051\n",
      "iteration=20300, training loss=0.007962823224952445\n",
      "iteration=20400, training loss=0.005214694783935556\n",
      "iteration=20500, training loss=0.018175357730942778\n",
      "iteration=20600, training loss=0.027705079569714144\n",
      "iteration=20700, training loss=0.01710572066134773\n",
      "iteration=20800, training loss=0.006814298196695745\n",
      "iteration=20900, training loss=0.00790727862258791\n",
      "iteration=21000, training loss=0.017960993935412263\n",
      "iteration=21100, training loss=0.01513529152405681\n",
      "iteration=21200, training loss=0.012565339048742317\n",
      "iteration=21300, training loss=0.015127919819497038\n",
      "iteration=21400, training loss=0.011344818729849066\n",
      "iteration=21500, training loss=0.005988364542718045\n",
      "iteration=21600, training loss=0.007820988513558405\n",
      "iteration=21700, training loss=0.005159748951700749\n",
      "iteration=21800, training loss=0.004426156436966267\n",
      "iteration=21900, training loss=0.007283071674755774\n",
      "iteration=22000, training loss=0.007239451097557321\n",
      "iteration=22100, training loss=0.012235517960798461\n",
      "iteration=22200, training loss=0.005891699069470633\n",
      "iteration=22300, training loss=0.014752490429964382\n",
      "iteration=22400, training loss=0.016743251566949766\n",
      "iteration=22500, training loss=0.009635331934841816\n",
      "iteration=22600, training loss=0.0059671202022582295\n",
      "iteration=22700, training loss=0.002674740020520403\n",
      "iteration=22800, training loss=0.007937166337942472\n",
      "iteration=22900, training loss=0.018805932886898517\n",
      "iteration=23000, training loss=0.009218241494527319\n",
      "iteration=23100, training loss=0.009407783504138933\n",
      "iteration=23200, training loss=0.0051037959521636365\n",
      "iteration=23300, training loss=0.00409327598346863\n",
      "iteration=23400, training loss=0.0061929753495496695\n",
      "iteration=23500, training loss=0.012097101004765137\n",
      "iteration=23600, training loss=0.006289292951405514\n",
      "iteration=23700, training loss=0.004807690193483722\n",
      "iteration=23800, training loss=0.005296069561591139\n",
      "iteration=23900, training loss=0.003080090217627003\n",
      "iteration=24000, training loss=0.006344425741408486\n",
      "Evaluating ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b53db95566440dd9a5c523c5274fc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21762 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc=78.41191066997519\n",
      "iteration=24100, training loss=0.0041843278774467765\n",
      "iteration=24200, training loss=0.006627498081943486\n",
      "iteration=24300, training loss=0.005200348621438025\n",
      "iteration=24400, training loss=0.011548903546645306\n",
      "iteration=24500, training loss=0.004167386148983496\n",
      "iteration=24600, training loss=0.003562556222459534\n",
      "iteration=24700, training loss=0.002099198045762023\n",
      "iteration=24800, training loss=0.006041368835358298\n",
      "iteration=24900, training loss=0.016999381499772426\n",
      "iteration=25000, training loss=0.015294488102954347\n",
      "iteration=25100, training loss=0.006250600880302954\n",
      "iteration=25200, training loss=0.005445035878074123\n",
      "iteration=25300, training loss=0.005544427602289943\n",
      "iteration=25400, training loss=0.00301401412289124\n",
      "iteration=25500, training loss=0.004324602580818464\n",
      "iteration=25600, training loss=0.00800437037149095\n",
      "iteration=25700, training loss=0.005398110161331715\n",
      "iteration=25800, training loss=0.0043092650998733\n",
      "iteration=25900, training loss=0.006843789588456275\n",
      "iteration=26000, training loss=0.007452548158762511\n",
      "iteration=26100, training loss=0.0022005477938364494\n",
      "iteration=26200, training loss=0.002670697257563006\n",
      "iteration=26300, training loss=0.0027476959719206205\n",
      "iteration=26400, training loss=0.005450751229436719\n",
      "iteration=26500, training loss=0.006740612510839128\n",
      "iteration=26600, training loss=0.0031384221328335116\n",
      "iteration=26700, training loss=0.006058376855653478\n",
      "iteration=26800, training loss=0.0036027136569464348\n",
      "iteration=26900, training loss=0.006108809338984429\n",
      "iteration=27000, training loss=0.004218094284951803\n",
      "iteration=27100, training loss=0.0031116529662540415\n",
      "iteration=27200, training loss=0.006184048741779406\n",
      "iteration=27300, training loss=0.008582210784297786\n",
      "iteration=27400, training loss=0.00784207570453873\n",
      "iteration=27500, training loss=0.003908162684601848\n",
      "iteration=27600, training loss=0.004323427447525318\n",
      "iteration=27700, training loss=0.003160235025388829\n",
      "iteration=27800, training loss=0.005590156802427373\n",
      "iteration=27900, training loss=0.0046766310611565136\n",
      "iteration=28000, training loss=0.002202023895515595\n",
      "iteration=28100, training loss=0.006135683509564842\n",
      "iteration=28200, training loss=0.004018846209946787\n",
      "iteration=28300, training loss=0.0062126566654478665\n",
      "iteration=28400, training loss=0.0017850392636319158\n",
      "iteration=28500, training loss=0.0019403925062579219\n",
      "iteration=28600, training loss=0.003634256423392799\n",
      "iteration=28700, training loss=0.003908407507879019\n",
      "iteration=28800, training loss=0.002131220109949936\n",
      "iteration=28900, training loss=0.002089286085247295\n",
      "iteration=29000, training loss=0.0029906450638009117\n",
      "iteration=29100, training loss=0.001527815691079013\n",
      "iteration=29200, training loss=0.0015113574790302665\n",
      "iteration=29300, training loss=0.0015968203354714205\n",
      "iteration=29400, training loss=0.00268519476048823\n",
      "iteration=29500, training loss=0.004035533077912987\n",
      "iteration=29600, training loss=0.001927861942713207\n",
      "iteration=29700, training loss=0.0034347152755071875\n",
      "iteration=29800, training loss=0.0025244962907163427\n",
      "iteration=29900, training loss=0.00249099507680512\n",
      "iteration=30000, training loss=0.0027430182193711517\n",
      "iteration=30100, training loss=0.0017578230806975625\n",
      "iteration=30200, training loss=0.002077453809324652\n",
      "iteration=30300, training loss=0.0063880900202275374\n",
      "iteration=30400, training loss=0.003039172853532364\n",
      "iteration=30500, training loss=0.0016738857466407353\n",
      "iteration=30600, training loss=0.001752901330764871\n",
      "iteration=30700, training loss=0.002636233940938837\n",
      "iteration=30800, training loss=0.0017303708089457359\n",
      "iteration=30900, training loss=0.0018263392798689893\n",
      "iteration=31000, training loss=0.0014017321940264082\n",
      "iteration=31100, training loss=0.0013848693964609992\n",
      "iteration=31200, training loss=0.0012255507503505215\n",
      "iteration=31300, training loss=0.000277188328691409\n",
      "iteration=31400, training loss=0.0010008963015934568\n",
      "iteration=31500, training loss=0.000958718341335043\n",
      "iteration=31600, training loss=0.0003242912123096176\n",
      "iteration=31700, training loss=0.000362628023212892\n",
      "iteration=31800, training loss=0.0005074607725691749\n",
      "iteration=31900, training loss=0.000756704210780299\n",
      "iteration=32000, training loss=0.0008188891365171003\n",
      "Evaluating ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037be31e00974459a09b1bd7258194e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21762 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc=80.07536072052201\n",
      "iteration=32100, training loss=0.00017208765177201713\n",
      "iteration=32200, training loss=0.0007422063836565939\n",
      "iteration=32300, training loss=0.0004020487438538112\n",
      "iteration=32400, training loss=0.0003862621966072766\n",
      "iteration=32500, training loss=0.0006595596306578955\n",
      "iteration=32600, training loss=0.0023221824111533352\n",
      "iteration=32700, training loss=0.0011913734080553696\n",
      "iteration=32800, training loss=0.0016881525178541778\n",
      "iteration=32900, training loss=0.0008082600909256144\n",
      "iteration=33000, training loss=0.000774270222245832\n",
      "iteration=33100, training loss=0.00020576856652041898\n",
      "iteration=33200, training loss=0.00043186217475522427\n",
      "iteration=33300, training loss=0.0004390942064492265\n",
      "iteration=33400, training loss=0.00019205623892048606\n",
      "iteration=33500, training loss=0.000996286592580873\n",
      "iteration=33600, training loss=0.0002101363630936248\n",
      "iteration=33700, training loss=0.0006230815353046637\n",
      "iteration=33800, training loss=0.00043497392267454415\n",
      "iteration=33900, training loss=0.0001828721136917011\n",
      "iteration=34000, training loss=0.0003389262857672293\n",
      "iteration=34100, training loss=0.00014703740820550592\n",
      "iteration=34200, training loss=0.0006529694215169001\n",
      "iteration=34300, training loss=0.0006884999126850744\n",
      "iteration=34400, training loss=0.0007386805286660092\n",
      "iteration=34500, training loss=0.0013337040771148167\n",
      "iteration=34600, training loss=0.0007736177666265576\n",
      "iteration=34700, training loss=0.00021364383761465433\n",
      "iteration=34800, training loss=0.00033529697504491195\n",
      "iteration=34900, training loss=0.00014743485398867052\n",
      "iteration=35000, training loss=0.00015777142649312737\n",
      "iteration=35100, training loss=0.0007033650818266323\n",
      "iteration=35200, training loss=0.00031869791160715977\n",
      "iteration=35300, training loss=0.00024310456967214123\n",
      "iteration=35400, training loss=0.00030992715066531675\n",
      "iteration=35500, training loss=0.0013007411262697132\n",
      "iteration=35600, training loss=0.000217570754357439\n",
      "iteration=35700, training loss=0.001204336215123476\n",
      "iteration=35800, training loss=0.0001447234545230458\n",
      "iteration=35900, training loss=0.000831195047176152\n",
      "iteration=36000, training loss=0.0007759466533025261\n",
      "iteration=36100, training loss=0.00034529726781329373\n",
      "iteration=36200, training loss=0.0002523782860407664\n",
      "iteration=36300, training loss=0.00041205249948689015\n",
      "iteration=36400, training loss=0.00018845597212930443\n",
      "iteration=36500, training loss=0.0004311206332567963\n",
      "iteration=36600, training loss=0.00039377111079375027\n",
      "iteration=36700, training loss=0.0008582182577629283\n",
      "iteration=36800, training loss=0.0005550017010318698\n",
      "iteration=36900, training loss=0.0007532742994226283\n",
      "iteration=37000, training loss=0.00021322929103916976\n",
      "iteration=37100, training loss=0.00022131970761620322\n",
      "iteration=37200, training loss=0.0004270933286898071\n",
      "iteration=37300, training loss=0.0003833325188315939\n",
      "iteration=37400, training loss=0.0006133168602718797\n",
      "iteration=37500, training loss=0.00032224883638264144\n",
      "iteration=37600, training loss=0.000853898312398087\n",
      "iteration=37700, training loss=0.0012785604646887805\n",
      "iteration=37800, training loss=0.00019704902271769242\n",
      "iteration=37900, training loss=0.000207686973244563\n",
      "iteration=38000, training loss=0.0010766711440373911\n",
      "iteration=38100, training loss=0.0008064696656583692\n",
      "iteration=38200, training loss=0.00012143749831011519\n",
      "iteration=38300, training loss=0.00025284544070018454\n",
      "iteration=38400, training loss=0.00015120112080694526\n",
      "iteration=38500, training loss=0.00125640132031549\n",
      "iteration=38600, training loss=0.0002891408217510616\n",
      "iteration=38700, training loss=0.0002012126022236771\n",
      "iteration=38800, training loss=0.00018567492323199987\n",
      "iteration=38900, training loss=0.00022352926429448417\n",
      "iteration=39000, training loss=0.00015640152410924203\n",
      "iteration=39100, training loss=0.0005244939319527475\n",
      "iteration=39200, training loss=0.0004540745745543973\n",
      "iteration=39300, training loss=0.0002057298468753288\n",
      "iteration=39400, training loss=0.000718076468911022\n",
      "iteration=39500, training loss=0.00035963703670859103\n",
      "iteration=39600, training loss=0.00021786078566947255\n",
      "iteration=39700, training loss=0.00038922839967199253\n",
      "iteration=39800, training loss=0.0002690681289095664\n",
      "iteration=39900, training loss=0.00032245031776255927\n",
      "iteration=40000, training loss=0.0007085955080401618\n",
      "Evaluating ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699bc6ee0fa9407e8ce958492dd669cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21762 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m total_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data)\n\u001b[1;32m      3\u001b[0m final_data, final_data_dev \u001b[39m=\u001b[39m data[:total_len\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m], data[total_len\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m:]\n\u001b[0;32m----> 4\u001b[0m trainer \u001b[39m=\u001b[39m Train(final_data, final_data_dev, \u001b[39m\"\u001b[39;49m\u001b[39mt5-small\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m, in \u001b[0;36mTrain.__init__\u001b[0;34m(self, data, data_val, model_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mback_propogate\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[0;32mIn[12], line 109\u001b[0m, in \u001b[0;36mTrain.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m     scalar\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m%\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_every\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[0;32m--> 109\u001b[0m     acc\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval(i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    110\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mvalidation acc=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(acc))\n\u001b[1;32m    112\u001b[0m     torch\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mstate_dict(),\n\u001b[1;32m    113\u001b[0m            join(lcquad2_dir,\u001b[39m'\u001b[39m\u001b[39mcheckpoints\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m_checkpoint\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[12], line 74\u001b[0m, in \u001b[0;36mTrain.val\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m     70\u001b[0m progress_bar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_function(inp,label)\n\u001b[0;32m---> 74\u001b[0m output\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     75\u001b[0m           num_beams\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,attention_mask\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m], \\\n\u001b[1;32m     76\u001b[0m             early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,output_attentions\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     78\u001b[0m out\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mbatch_decode(output,skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     80\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(out)):\n\u001b[1;32m     81\u001b[0m     \u001b[39m#print(out[k].replace('<pad>','').replace('</s>','').strip())\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/generation/utils.py:1524\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1523\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1524\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1525\u001b[0m         input_ids,\n\u001b[1;32m   1526\u001b[0m         beam_scorer,\n\u001b[1;32m   1527\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1528\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1529\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1530\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1531\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1532\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1533\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1534\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1535\u001b[0m     )\n\u001b[1;32m   1537\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1538\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/generation/utils.py:2810\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2806\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2808\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2810\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2811\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2812\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2813\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2814\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2815\u001b[0m )\n\u001b[1;32m   2817\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2818\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1716\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1713\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1715\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> 1716\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1717\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1718\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1719\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1720\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1721\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   1722\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1723\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1724\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1725\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1726\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1727\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1728\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1729\u001b[0m )\n\u001b[1;32m   1731\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1733\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1086\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m   1074\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1075\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     )\n\u001b[1;32m   1085\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1086\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1087\u001b[0m         hidden_states,\n\u001b[1;32m   1088\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1089\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   1090\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1091\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1092\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   1093\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1094\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1095\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1096\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1097\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1098\u001b[0m     )\n\u001b[1;32m   1100\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:693\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 693\u001b[0m self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m0\u001b[39;49m](\n\u001b[1;32m    694\u001b[0m     hidden_states,\n\u001b[1;32m    695\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    696\u001b[0m     position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    697\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    698\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    699\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    700\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    701\u001b[0m )\n\u001b[1;32m    702\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m self_attention_outputs[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    703\u001b[0m attention_outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = df.values.tolist()\n",
    "total_len = len(data)\n",
    "final_data, final_data_dev = data[:total_len//10], data[total_len//10:]\n",
    "trainer = Train(final_data, final_data_dev, \"t5-small\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
