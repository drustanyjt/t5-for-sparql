{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text2SPARQL\n",
    "\n",
    "This is a development workbook for getting the hang of training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"lcquad2\"\n",
    "DATASET_FOLDER = \"data\"\n",
    "DATASET_PATH = os.path.join(DATASET_FOLDER, DATASET_NAME)\n",
    "\n",
    "ACCELERATE_USE = False \n",
    "ACCELERATE_STR = \"-accelerate\" if ACCELERATE_USE else \"\"\n",
    "\n",
    "MODEL_NAME = \"t5-small\" # With t5-small, the non accelerated training works better than accelerated?\n",
    "MODEL_TYPE = \"text2sparql\"\n",
    "MODEL_FULL = f\"{MODEL_TYPE}-{MODEL_NAME}-{DATASET_NAME}{ACCELERATE_STR}\"\n",
    "\n",
    "MODEL_FOLDER = \"models\"\n",
    "MODEL_PATH = os.path.join(MODEL_FOLDER, MODEL_FULL)\n",
    "\n",
    "EVALUATION_FOLDER = os.path.join(MODEL_FOLDER, \"evaluations\")\n",
    "CHECKPOINT_FOLDER = os.path.join(MODEL_FOLDER, \"checkpoints\")\n",
    "\n",
    "folders = [MODEL_FOLDER, EVALUATION_FOLDER, CHECKPOINT_FOLDER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(DATASET_PATH)\n",
    "\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Bannerjee does some preprocessing of the LCQuAD dataset,\n",
    "I try to replicate that here.\n",
    "\n",
    "First we load some files into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chandrasekhar limit', 'toluene', 'Olympic victor, stadion']\n",
      "[\"Who is the child of Ranavalona I's husband?\",\n",
      " 'SELECT ?answer WHERE { wd:Q169794 wdt:P26 ?X . ?X wdt:P22 ?answer}']\n",
      "['video', 'head of government']\n",
      "['(', 'rdfs:label', 'by', 'ask']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from os.path import join\n",
    "from pprint import pprint\n",
    "\n",
    "assert DATASET_PATH.endswith(\"lcquad2\")\n",
    "lcquad2_dir = DATASET_PATH\n",
    "\n",
    "# LCQuAD2 entity labels\n",
    "with open(join(lcquad2_dir, \"lcq2_labels.pickle\"), \"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "pprint(list(labels[k] for k in ['q51366', 'q15779', 'q23906217']))\n",
    "\n",
    "# Training Data has exactly the same file size as the official one\n",
    "with open(join(lcquad2_dir, \"train.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "pprint(list(data[1][k] for k in [\"question\", \"sparql_wikidata\"]))\n",
    "\n",
    "# Load the relation labels\n",
    "with open(join(lcquad2_dir, \"relations.json\")) as f:\n",
    "    rel_labels = json.load(f)\n",
    "\n",
    "pprint(list(rel_labels[k] for k in [\"P10\", \"P6\"]))\n",
    "\n",
    "# Load the sparql vocabulary\n",
    "with open(join(lcquad2_dir, \"vocab.txt\")) as f:\n",
    "    vocab = list(map(lambda x: x.strip(), f.readlines()))\n",
    "    vocab.append('null') # not too sure what this is for\n",
    "\n",
    "pprint(vocab[1:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some labels are missing from the lcq2_labels.pickle,\n",
    "and cause run time errors in the script.\n",
    "We add them back here to avoid this problem\n",
    "(though ideally we should find a better label to entity map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['quercia']='null'\n",
    "labels['qui']='null'\n",
    "labels['}']='null'\n",
    "labels['p5122'] = 'Ontario public library ID'.lower()\n",
    "labels['p3888']='Boijmans artist ID'\n",
    "labels['p5388']='Bulgarian Antarctic Gazetteer ID'\n",
    "labels['p5151']='Israel Film Fund ID'\n",
    "labels['p3633']='British Museum place ID'\n",
    "labels['p1733']='Steam application ID'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we assign vocabularies to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<extra_id_0>', '<extra_id_60>', '<extra_id_16>']\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "for i, text in enumerate(vocab):\n",
    "    vocab_dict[text] = f'<extra_id_{i}>'\n",
    "\n",
    "pprint([vocab_dict[k] for k in ['\"', 'null', '?value']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And adjust some labels to use the null token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in labels:\n",
    "    if labels[k] is None:\n",
    "        labels[k] = vocab_dict['null']\n",
    "        # print(f'{k}: {labels[k]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xprint(thing):\n",
    "    pprint(thing)\n",
    "    return thing\n",
    "\n",
    "def compare(x, y=None):\n",
    "\n",
    "    def _compare(z):\n",
    "        pprint(f\"Old: {x}\")\n",
    "        pprint(f\"New: {z}\")\n",
    "    \n",
    "    if not y:\n",
    "        return lambda z : _compare(z)\n",
    "    else:\n",
    "        return lambda : _compare(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reformate the dataset\n",
    "- Note: it seems that Bannerjee replaces training data\n",
    "that has no questions with the Natural Language version.\n",
    "\n",
    "For reference these are the definition of each feature,\n",
    "taken **verbatim** from their [homepage](https://sda.tech/projects/lc-quad-2/)\n",
    "```\n",
    "{\n",
    "     \"uid\": a unique id number\n",
    "     \"sparql_wikidata\": a sparql fro wikidata endpoint\n",
    "     \"sparql_dbpedia18\": a sparql for DBpedia endpoint which has wikidata information\n",
    "     \"NNQT_question\": system generated question,\n",
    "     \"question\": Verbalised question,\n",
    "     \"paraphrased_question\": paraphrased version of the verbalised question,\n",
    "     \"template_id\": id for the template\n",
    "     \"template\": template discription    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data_x, data_y = [], []\n",
    "data_x_shuffle = []\n",
    "\n",
    "for i, inst in enumerate(data):\n",
    "    wikisparql = inst['sparql_wikidata']\n",
    "    if inst['question'] is None:\n",
    "        question = inst['NNQT_question']\n",
    "    else:\n",
    "        question = inst['question']\n",
    "    question = question.replace('{', '').replace('}', '')\n",
    "\n",
    "    match_str = r\"\\'(.*?)\\'\"\n",
    "    hashi = {}\n",
    "    # To mask filter literals\n",
    "    if re.search(match_str, wikisparql):\n",
    "        lits=re.findall(match_str,wikisparql)\n",
    "        # print(f\"Old: {wikisparql}\")\n",
    "        for j, lit in enumerate(lits):\n",
    "            idx = j + 1\n",
    "            wikisparql = wikisparql.replace(f\"'{lit.strip()}'\", f\"'###{idx}'\")\n",
    "            hashi[f'###{idx}'] = lit.strip()\n",
    "        # print(f\"New: {wikisparql}\")\n",
    "    \n",
    "    # there is an extra space beacuse of http: and https:\n",
    "    sparql = wikisparql.replace('(',' ( ').replace(')',' ) ') \\\n",
    "    .replace('{',' { '). \\\n",
    "    replace('}',' } ').replace('wd:','wd: ').replace('wdt:','wdt: '). \\\n",
    "    replace(' p:',' p: ').replace(' ps:',' ps: ').replace('pq:','pq: '). \\\n",
    "    replace(',',' , ').replace(\",'\",\", '\").replace(\"'\",\" ' \").replace('.',' . '). \\\n",
    "    replace('=',' = ').replace('  ',' ').lower()\n",
    "    \n",
    "    # print(f\"sparql: {sparql}\")\n",
    "    # select distinct ?obj where { wd: q188920 wdt: p2813 ?obj . ?obj wdt: p31 wd: q1002697 } \n",
    "\n",
    "    _ents = re.findall( r'wd: (?:.*?) ', sparql) # ['wd: q188920 ', 'wd: q1002697 ']\n",
    "    _ents_for_labels = re.findall( r'wd: (.*?) ', sparql) # ['q188920', 'q1002697']\n",
    "    \n",
    "    _rels = re.findall( r'wdt: (?:.*?) ',sparql)\n",
    "    _rels += re.findall( r' p: (?:.*?) ',sparql)\n",
    "    _rels += re.findall( r' ps: (?:.*?) ',sparql)\n",
    "    _rels += re.findall( r'pq: (?:.*?) ',sparql) # ['wdt: p2813 ', 'wdt: p31 ']\n",
    "    # Missing rdfs:label, not sure if that is important\n",
    "    \n",
    "    _rels_for_labels = re.findall( r'wdt: (.*?) ',sparql)\n",
    "    _rels_for_labels += re.findall( r' p: (.*?) ',sparql)\n",
    "    _rels_for_labels += re.findall( r' ps: (.*?) ',sparql)\n",
    "    _rels_for_labels += re.findall( r'pq: (.*?) ',sparql) # ['p2813', 'p31']\n",
    "\n",
    "    # print(_rels)\n",
    "    # print(_rels_for_labels)\n",
    "    for j in range(len(_ents_for_labels)):\n",
    "        # print('Q'+_ents_for_labels[j][1:])\n",
    "        if '}' in _ents[j]: # Entry 12686 is malformed\n",
    "            # pprint(inst)\n",
    "            # pprint(_ents)\n",
    "            _ents[j]=''\n",
    "        _ents[j]=_ents[j]+labels[_ents_for_labels[j]]+' '\n",
    "        # wd: q36970 -> wd: q36970 Jackie Chan\n",
    "\n",
    "    for j in range(len(_rels_for_labels)):\n",
    "        if _rels_for_labels[j].upper() not in rel_labels:\n",
    "            # For some reasons the original preprocess.py didnt convert to upper?\n",
    "            rel_labels['P'+_rels_for_labels[j][1:]]=vocab_dict['null']\n",
    "        _rels[j]=_rels[j]+rel_labels['P'+_rels_for_labels[j][1:]]+' '\n",
    "        # wdt: p26 -> wdt: p26 spouse\n",
    "    # print(_ents)\n",
    "\n",
    "    _ents+=_rels\n",
    "    # random.shuffle(_ents)\n",
    "    # random.shuffle(_rels)\n",
    "\n",
    "    # move to a function\n",
    "    newvars = ['?vr0','?vr1','?vr2','?vr3','?vr4','?vr5']\n",
    "    sparql_split = sparql.split()\n",
    "    variables = set([x for x in sparql_split if x[0] == '?'])\n",
    "    for j, var in enumerate(sorted(variables)):\n",
    "        if var == '?maskvar1': #???\n",
    "            print(sparql)\n",
    "            continue\n",
    "        sparql = sparql.replace(var, newvars[j]) # Normalize var names\n",
    "    \n",
    "    # old = compare(sparql)\n",
    "\n",
    "    split = sparql.split()\n",
    "    \n",
    "    for j, item in enumerate(split):\n",
    "        if item in vocab_dict:\n",
    "            split[j] = vocab_dict[item]\n",
    "    \n",
    "    split = ' '.join(split).strip()\n",
    "    # old(split)\n",
    "\n",
    "    for keys in hashi:\n",
    "        split = split.replace(keys, hashi[keys])\n",
    "    \n",
    "    data_y.append(split)\n",
    "\n",
    "    for rel in _ents:\n",
    "        rel=rel.replace('wd:',vocab_dict['wd:']+' ')\n",
    "        rel=rel.replace('wdt:',vocab_dict['wdt:']+' ')\n",
    "        old = compare(rel)\n",
    "        if 'p:' in rel:\n",
    "            if 'http' in rel:\n",
    "                print(inst) # There are no more http\n",
    "            rel=rel.replace('p:',vocab_dict['p:']+' ')\n",
    "            # old(rel)\n",
    "        rel=rel.replace('ps:',vocab_dict['ps:']+' ')\n",
    "        rel=rel.replace('pq:',vocab_dict['pq:']+' ')\n",
    "        question=question+' '+vocab_dict['[DEF]']+' '+rel\n",
    "    data_x.append(question.strip())\n",
    "\n",
    "assert len(data_x) == len(data_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'x': data_x,\n",
    "    'y': data_y,\n",
    "    })\n",
    "\n",
    "save_file = join(lcquad2_dir, 'preprocessed_data.csv')\n",
    "df.to_csv(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What periodical literature does Delta Air Line...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_21&gt; &lt;extra_id_39&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is the child of Ranavalona I's husband? &lt;e...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_39&gt; &lt;extra_id_19&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is it true Jeff_Bridges occupation Lane Chandl...</td>\n",
       "      <td>&lt;extra_id_4&gt; &lt;extra_id_19&gt; &lt;extra_id_33&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the pre-requisite of phase matter of G...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_39&gt; &lt;extra_id_19&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which is the operating income for Qantas? &lt;ext...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_21&gt; &lt;extra_id_39&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   x  \\\n",
       "0  What periodical literature does Delta Air Line...   \n",
       "1  Who is the child of Ranavalona I's husband? <e...   \n",
       "2  Is it true Jeff_Bridges occupation Lane Chandl...   \n",
       "3  What is the pre-requisite of phase matter of G...   \n",
       "4  Which is the operating income for Qantas? <ext...   \n",
       "\n",
       "                                                   y  \n",
       "0  <extra_id_6> <extra_id_21> <extra_id_39> <extr...  \n",
       "1  <extra_id_6> <extra_id_39> <extra_id_19> <extr...  \n",
       "2  <extra_id_4> <extra_id_19> <extra_id_33> <extr...  \n",
       "3  <extra_id_6> <extra_id_39> <extra_id_19> <extr...  \n",
       "4  <extra_id_6> <extra_id_21> <extra_id_39> <extr...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Now we need to generate a T5 model for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import transformers\n",
    "# from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")\n",
    "        pprint(self.model.hf_device_map)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        outputs = self.model(\n",
    "            input_ids = input['input_ids'],\n",
    "            labels = input['labels'],\n",
    "            attention_mask = input['attention_mask'],\n",
    "            output_hidden_states = True,\n",
    "            output_attentions = True\n",
    "        )\n",
    "\n",
    "        return outputs.loss\n",
    "\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\") # Device_map splits the load over multiple GPUs, this seems to be quite new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class Train:\n",
    "    def __init__(self,data,data_val, model_name):\n",
    "        self.data=data\n",
    "        self.dev_data=data_val\n",
    "\n",
    "        self.tokenizer=T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model=Model(model_name)\n",
    "        # self.model.to(f'cuda:{self.model.device_ids[0]}')  \n",
    "           \n",
    "        # Modify lr?\n",
    "        self.optimizer=optim.AdamW(self.model.parameters(),lr=0.0015)\n",
    "        self.lr_scheduler=transformers. \\\n",
    "        get_polynomial_decay_schedule_with_warmup(self.optimizer, 5000, 30000,power=0.5)\n",
    "\n",
    "        self.iters=60000\n",
    "        self.print_every=100\n",
    "        self.eval_every=8000\n",
    "        # self.num_gpus=1\n",
    "        self.eval_bs=6\n",
    "        self.bs=5\n",
    "        self.back_propogate=10\n",
    "        \n",
    "        self.train()\n",
    "\n",
    "    def generate_batch(self):\n",
    "        output=random.sample(self.data,self.bs)\n",
    "        inp,label=[],[]\n",
    "        for dat in output:\n",
    "            inp.append(dat[0])\n",
    "            label.append(dat[1])\n",
    "\n",
    "        return inp,label\n",
    "\n",
    "    def preprocess_function(self,inputs, targets):\n",
    "        model_inputs=self.tokenizer(inputs, padding=True, \\\n",
    "                        return_tensors='pt',max_length=512, truncation=True)\n",
    "        labels=self.tokenizer(targets,padding=True,max_length=512, truncation=True)\n",
    "\n",
    "        if True:\n",
    "            labels[\"input_ids\"] = [\n",
    "            [(l if l != self.tokenizer.pad_token_id else -100) \\\n",
    "             for l in label] for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "        labels['input_ids']=torch.tensor(labels['input_ids'])\n",
    "        model_inputs[\"labels\"]=labels[\"input_ids\"].to(0)\n",
    "        model_inputs[\"input_ids\"]=model_inputs[\"input_ids\"].to(0)\n",
    "        model_inputs[\"attention_mask\"]=model_inputs[\"attention_mask\"].to(0)\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    def val(self,o):\n",
    "        print('Evaluating ...')\n",
    "        self.model.eval()\n",
    "        acc,bs,i=0,self.eval_bs,0\n",
    "        saver=[]\n",
    "\n",
    "        progress_bar = tqdm.auto.tqdm(range(math.ceil(len(self.dev_data) / bs)))\n",
    "        progress_bar.set_description(f\"Eval {o}\")\n",
    "           \n",
    "        while i<len(self.dev_data):\n",
    "            bs_=min(bs,len(self.dev_data)-i)\n",
    "            if i % (2*bs_) < bs_:\n",
    "                print(f\"Evaluation {i}/{self.dev_data}\")\n",
    "            i+=bs_\n",
    "            inp,label=[],[]\n",
    "            for j in range(i-bs_,i):\n",
    "                inp.append(self.dev_data[j][0])\n",
    "                label.append(self.dev_data[j][1])\n",
    "            \n",
    "\n",
    "            input=self.preprocess_function(inp,label)\n",
    "\n",
    "            output=self.model.model.generate(input_ids=input['input_ids'],\n",
    "                      num_beams=10,attention_mask=input['attention_mask'], \\\n",
    "                        early_stopping=True, max_length=200,output_hidden_states=True,output_attentions=True)\n",
    "            \n",
    "            out=self.tokenizer.batch_decode(output,skip_special_tokens=False)\n",
    "\n",
    "            for k in range(len(out)):\n",
    "                #print(out[k].replace('<pad>','').replace('</s>','').strip())\n",
    "                a1=out[k].replace('<pad>','').replace('</s>','').replace('<unk>','').replace('<s>','').strip().replace(' ','')\n",
    "                a2=label[k].strip().replace(' ','')\n",
    "                #print(a1, '       ', a2)\n",
    "                saver.append({'input':inp[k],'gold':label[k].strip(),'generated':out[k].replace('<pad>',''). \\\n",
    "                      replace('</s>','').replace('<unk>','').replace('<s>','').strip()})\n",
    "                if a1==a2:\n",
    "                    acc+=1; #print('ttt')\n",
    "\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        file=open(join(EVALUATION_FOLDER, 'dev_result_'+str(o)+'.json'),'w')\n",
    "        json.dump(saver,file)\n",
    "        file.close()\n",
    "        return 100*acc/len(self.dev_data)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        scalar=0\n",
    "        for i in range(self.iters):\n",
    "            self.model.train()\n",
    "            inp,label=self.generate_batch()\n",
    "            input=self.preprocess_function(inp,label)\n",
    "            loss=self.model(input)\n",
    "\n",
    "            scalar+=loss.mean().item()\n",
    "            if(i+1)%self.print_every==0:\n",
    "                print('iteration={}, training loss={}'.format(i+1,scalar/self.print_every))\n",
    "                scalar=0\n",
    "            if(i + 1)%self.eval_every==0:\n",
    "                acc=self.val(i+1)\n",
    "                print('validation acc={}'.format(acc))\n",
    "\n",
    "                torch.save(self.model.state_dict(),\n",
    "                       join(CHECKPOINT_FOLDER,'checkpoint_'+str(i + 1)+'.pth'))\n",
    "            \n",
    "            loss/=self.back_propogate\n",
    "            loss.mean().backward()\n",
    "            if (i+1)%self.back_propogate:\n",
    "                self.optimizer.step();\n",
    "                self.lr_scheduler.step();\n",
    "                self.optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n",
      "Evaluating ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 0/3627 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 1/3627 [00:00<55:57,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 2/3627 [00:03<1:37:19,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 3/3627 [00:04<1:46:36,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 4/3627 [00:07<1:54:56,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 5/3627 [00:09<1:57:58,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 6/3627 [00:11<1:59:48,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 7/3627 [00:13<1:57:46,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 8/3627 [00:15<2:00:50,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 9/3627 [00:17<1:59:53,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 10/3627 [00:19<1:59:25,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 11/3627 [00:21<1:59:40,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 12/3627 [00:22<1:57:56,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 13/3627 [00:24<1:58:24,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 14/3627 [00:26<1:58:56,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 15/3627 [00:28<1:51:40,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 16/3627 [00:30<1:56:15,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 17/3627 [00:31<1:37:07,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   0%|          | 18/3627 [00:33<1:44:15,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 19/3627 [00:35<1:47:05,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 20/3627 [00:37<1:50:51,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 21/3627 [00:38<1:37:09,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 22/3627 [00:40<1:42:22,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 23/3627 [00:42<1:46:13,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 24/3627 [00:44<1:49:04,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 25/3627 [00:46<1:49:36,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 26/3627 [00:48<1:52:16,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 27/3627 [00:49<1:51:50,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 28/3627 [00:51<1:52:24,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 29/3627 [00:53<1:43:59,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 30/3627 [00:55<1:48:46,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 31/3627 [00:56<1:33:32,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 32/3627 [00:58<1:40:15,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 33/3627 [01:00<1:46:40,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 34/3627 [01:02<1:50:10,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 35/3627 [01:04<1:54:29,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 36/3627 [01:06<1:56:01,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 37/3627 [01:08<1:55:45,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 38/3627 [01:10<1:57:18,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 39/3627 [01:12<1:57:05,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 40/3627 [01:14<1:56:24,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 41/3627 [01:16<1:57:19,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 42/3627 [01:17<1:56:59,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 43/3627 [01:20<1:58:43,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 44/3627 [01:22<1:59:28,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|          | 45/3627 [01:24<1:58:31,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|▏         | 46/3627 [01:26<2:00:49,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|▏         | 47/3627 [01:26<1:38:50,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|▏         | 48/3627 [01:28<1:43:11,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|▏         | 49/3627 [01:30<1:45:49,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|▏         | 50/3627 [01:32<1:49:46,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|▏         | 51/3627 [01:34<1:50:05,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|▏         | 52/3627 [01:36<1:53:38,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|▏         | 53/3627 [01:38<1:53:56,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   1%|▏         | 54/3627 [01:40<1:55:10,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   2%|▏         | 55/3627 [01:42<1:55:05,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   2%|▏         | 56/3627 [01:44<1:55:03,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   2%|▏         | 57/3627 [01:46<1:55:23,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   2%|▏         | 58/3627 [01:47<1:46:20,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   2%|▏         | 59/3627 [01:49<1:48:37,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   2%|▏         | 60/3627 [01:51<1:52:46,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   2%|▏         | 61/3627 [01:53<1:55:04,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   2%|▏         | 62/3627 [01:55<1:55:34,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   2%|▏         | 63/3627 [01:57<1:55:18,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   2%|▏         | 64/3627 [01:59<1:57:19,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   2%|▏         | 65/3627 [02:01<1:56:23,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   2%|▏         | 66/3627 [02:03<1:54:43,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 1:   2%|▏         | 67/3627 [02:04<1:45:30,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 402\n"
     ]
    }
   ],
   "source": [
    "data = df.values.tolist()\n",
    "total_len = len(data)\n",
    "final_data, final_data_dev = data[:total_len//10], data[total_len//10:]\n",
    "trainer = Train(final_data, final_data_dev, \"t5-small\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
