{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text2SPARQL\n",
    "\n",
    "This is a development workbook for getting the hang of training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.cuda.is_available() -> bool>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Bannerjee does some preprocessing of the LCQuAD dataset,\n",
    "I try to replicate that here.\n",
    "\n",
    "First we load some files into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chandrasekhar limit', 'toluene', 'Olympic victor, stadion']\n",
      "[\"Who is the child of Ranavalona I's husband?\",\n",
      " 'SELECT ?answer WHERE { wd:Q169794 wdt:P26 ?X . ?X wdt:P22 ?answer}']\n",
      "['video', 'head of government']\n",
      "['(', 'rdfs:label', 'by', 'ask']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from os.path import join\n",
    "from pprint import pprint\n",
    "\n",
    "lcquad2_dir = os.path.join(\"baseline\", \"lcquad2\")\n",
    "\n",
    "# LCQuAD2 entity labels\n",
    "with open(join(lcquad2_dir, \"lcq2_labels.pickle\"), \"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "pprint(list(labels[k] for k in ['q51366', 'q15779', 'q23906217']))\n",
    "\n",
    "# Training Data has exactly the same file size as the official one\n",
    "with open(join(lcquad2_dir, \"train.json\")) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "pprint(list(data[1][k] for k in [\"question\", \"sparql_wikidata\"]))\n",
    "\n",
    "# Load the relation labels\n",
    "with open(join(lcquad2_dir, \"relations.json\")) as f:\n",
    "    rel_labels = json.load(f)\n",
    "\n",
    "pprint(list(rel_labels[k] for k in [\"P10\", \"P6\"]))\n",
    "\n",
    "# Load the sparql vocabulary\n",
    "with open(join(lcquad2_dir, \"vocab.txt\")) as f:\n",
    "    vocab = list(map(lambda x: x.strip(), f.readlines()))\n",
    "    vocab.append('null') # not too sure what this is for\n",
    "\n",
    "pprint(vocab[1:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some labels are missing from the lcq2_labels.pickle,\n",
    "and cause run time errors in the script.\n",
    "We add them back here to avoid this problem\n",
    "(though ideally we should find a better label to entity map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['quercia']='null'\n",
    "labels['qui']='null'\n",
    "labels['}']='null'\n",
    "labels['p5122'] = 'Ontario public library ID'.lower()\n",
    "labels['p3888']='Boijmans artist ID'\n",
    "labels['p5388']='Bulgarian Antarctic Gazetteer ID'\n",
    "labels['p5151']='Israel Film Fund ID'\n",
    "labels['p3633']='British Museum place ID'\n",
    "labels['p1733']='Steam application ID'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we assign vocabularies to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<extra_id_0>', '<extra_id_60>', '<extra_id_16>']\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "for i, text in enumerate(vocab):\n",
    "    vocab_dict[text] = f'<extra_id_{i}>'\n",
    "\n",
    "pprint([vocab_dict[k] for k in ['\"', 'null', '?value']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And adjust some labels to use the null token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in labels:\n",
    "    if labels[k] is None:\n",
    "        labels[k] = vocab_dict['null']\n",
    "        # print(f'{k}: {labels[k]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xprint(thing):\n",
    "    pprint(thing)\n",
    "    return thing\n",
    "\n",
    "def compare(x, y=None):\n",
    "\n",
    "    def _compare(z):\n",
    "        pprint(f\"Old: {x}\")\n",
    "        pprint(f\"New: {z}\")\n",
    "    \n",
    "    if not y:\n",
    "        return lambda z : _compare(z)\n",
    "    else:\n",
    "        return lambda : _compare(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reformate the dataset\n",
    "- Note: it seems that Bannerjee replaces training data\n",
    "that has no questions with the Natural Language version.\n",
    "\n",
    "For reference these are the definition of each feature,\n",
    "taken **verbatim** from their [homepage](https://sda.tech/projects/lc-quad-2/)\n",
    "```\n",
    "{\n",
    "     \"uid\": a unique id number\n",
    "     \"sparql_wikidata\": a sparql fro wikidata endpoint\n",
    "     \"sparql_dbpedia18\": a sparql for DBpedia endpoint which has wikidata information\n",
    "     \"NNQT_question\": system generated question,\n",
    "     \"question\": Verbalised question,\n",
    "     \"paraphrased_question\": paraphrased version of the verbalised question,\n",
    "     \"template_id\": id for the template\n",
    "     \"template\": template discription    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data_x, data_y = [], []\n",
    "data_x_shuffle = []\n",
    "\n",
    "for i, inst in enumerate(data):\n",
    "    wikisparql = inst['sparql_wikidata']\n",
    "    if inst['question'] is None:\n",
    "        question = inst['NNQT_question']\n",
    "    else:\n",
    "        question = inst['question']\n",
    "    question = question.replace('{', '').replace('}', '')\n",
    "\n",
    "    match_str = r\"\\'(.*?)\\'\"\n",
    "    hashi = {}\n",
    "    # To mask filter literals\n",
    "    if re.search(match_str, wikisparql):\n",
    "        lits=re.findall(match_str,wikisparql)\n",
    "        # print(f\"Old: {wikisparql}\")\n",
    "        for j, lit in enumerate(lits):\n",
    "            idx = j + 1\n",
    "            wikisparql = wikisparql.replace(f\"'{lit.strip()}'\", f\"'###{idx}'\")\n",
    "            hashi[f'###{idx}'] = lit.strip()\n",
    "        # print(f\"New: {wikisparql}\")\n",
    "    \n",
    "    # there is an extra space beacuse of http: and https:\n",
    "    sparql = wikisparql.replace('(',' ( ').replace(')',' ) ') \\\n",
    "    .replace('{',' { '). \\\n",
    "    replace('}',' } ').replace('wd:','wd: ').replace('wdt:','wdt: '). \\\n",
    "    replace(' p:',' p: ').replace(' ps:',' ps: ').replace('pq:','pq: '). \\\n",
    "    replace(',',' , ').replace(\",'\",\", '\").replace(\"'\",\" ' \").replace('.',' . '). \\\n",
    "    replace('=',' = ').replace('  ',' ').lower()\n",
    "    \n",
    "    # print(f\"sparql: {sparql}\")\n",
    "    # select distinct ?obj where { wd: q188920 wdt: p2813 ?obj . ?obj wdt: p31 wd: q1002697 } \n",
    "\n",
    "    _ents = re.findall( r'wd: (?:.*?) ', sparql) # ['wd: q188920 ', 'wd: q1002697 ']\n",
    "    _ents_for_labels = re.findall( r'wd: (.*?) ', sparql) # ['q188920', 'q1002697']\n",
    "    \n",
    "    _rels = re.findall( r'wdt: (?:.*?) ',sparql)\n",
    "    _rels += re.findall( r' p: (?:.*?) ',sparql)\n",
    "    _rels += re.findall( r' ps: (?:.*?) ',sparql)\n",
    "    _rels += re.findall( r'pq: (?:.*?) ',sparql) # ['wdt: p2813 ', 'wdt: p31 ']\n",
    "    # Missing rdfs:label, not sure if that is important\n",
    "    \n",
    "    _rels_for_labels = re.findall( r'wdt: (.*?) ',sparql)\n",
    "    _rels_for_labels += re.findall( r' p: (.*?) ',sparql)\n",
    "    _rels_for_labels += re.findall( r' ps: (.*?) ',sparql)\n",
    "    _rels_for_labels += re.findall( r'pq: (.*?) ',sparql) # ['p2813', 'p31']\n",
    "\n",
    "    # print(_rels)\n",
    "    # print(_rels_for_labels)\n",
    "    for j in range(len(_ents_for_labels)):\n",
    "        # print('Q'+_ents_for_labels[j][1:])\n",
    "        if '}' in _ents[j]: # Entry 12686 is malformed\n",
    "            # pprint(inst)\n",
    "            # pprint(_ents)\n",
    "            _ents[j]=''\n",
    "        _ents[j]=_ents[j]+labels[_ents_for_labels[j]]+' '\n",
    "        # wd: q36970 -> wd: q36970 Jackie Chan\n",
    "\n",
    "    for j in range(len(_rels_for_labels)):\n",
    "        if _rels_for_labels[j].upper() not in rel_labels:\n",
    "            # For some reasons the original preprocess.py didnt convert to upper?\n",
    "            rel_labels['P'+_rels_for_labels[j][1:]]=vocab_dict['null']\n",
    "        _rels[j]=_rels[j]+rel_labels['P'+_rels_for_labels[j][1:]]+' '\n",
    "        # wdt: p26 -> wdt: p26 spouse\n",
    "    # print(_ents)\n",
    "\n",
    "    _ents+=_rels\n",
    "    # random.shuffle(_ents)\n",
    "    # random.shuffle(_rels)\n",
    "\n",
    "    # move to a function\n",
    "    newvars = ['?vr0','?vr1','?vr2','?vr3','?vr4','?vr5']\n",
    "    sparql_split = sparql.split()\n",
    "    variables = set([x for x in sparql_split if x[0] == '?'])\n",
    "    for j, var in enumerate(sorted(variables)):\n",
    "        if var == '?maskvar1': #???\n",
    "            print(sparql)\n",
    "            continue\n",
    "        sparql = sparql.replace(var, newvars[j]) # Normalize var names\n",
    "    \n",
    "    # old = compare(sparql)\n",
    "\n",
    "    split = sparql.split()\n",
    "    \n",
    "    for j, item in enumerate(split):\n",
    "        if item in vocab_dict:\n",
    "            split[j] = vocab_dict[item]\n",
    "    \n",
    "    split = ' '.join(split).strip()\n",
    "    # old(split)\n",
    "\n",
    "    for keys in hashi:\n",
    "        split = split.replace(keys, hashi[keys])\n",
    "    \n",
    "    data_y.append(split)\n",
    "\n",
    "    for rel in _ents:\n",
    "        rel=rel.replace('wd:',vocab_dict['wd:']+' ')\n",
    "        rel=rel.replace('wdt:',vocab_dict['wdt:']+' ')\n",
    "        old = compare(rel)\n",
    "        if 'p:' in rel:\n",
    "            if 'http' in rel:\n",
    "                print(inst) # There are no more http\n",
    "            rel=rel.replace('p:',vocab_dict['p:']+' ')\n",
    "            # old(rel)\n",
    "        rel=rel.replace('ps:',vocab_dict['ps:']+' ')\n",
    "        rel=rel.replace('pq:',vocab_dict['pq:']+' ')\n",
    "        question=question+' '+vocab_dict['[DEF]']+' '+rel\n",
    "    data_x.append(question.strip())\n",
    "\n",
    "assert len(data_x) == len(data_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'x': data_x,\n",
    "    'y': data_y,\n",
    "    })\n",
    "\n",
    "save_file = join(lcquad2_dir, 'preprocessed_data.csv')\n",
    "df.to_csv(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What periodical literature does Delta Air Line...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_21&gt; &lt;extra_id_39&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is the child of Ranavalona I's husband? &lt;e...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_39&gt; &lt;extra_id_19&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is it true Jeff_Bridges occupation Lane Chandl...</td>\n",
       "      <td>&lt;extra_id_4&gt; &lt;extra_id_19&gt; &lt;extra_id_33&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the pre-requisite of phase matter of G...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_39&gt; &lt;extra_id_19&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which is the operating income for Qantas? &lt;ext...</td>\n",
       "      <td>&lt;extra_id_6&gt; &lt;extra_id_21&gt; &lt;extra_id_39&gt; &lt;extr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   x  \\\n",
       "0  What periodical literature does Delta Air Line...   \n",
       "1  Who is the child of Ranavalona I's husband? <e...   \n",
       "2  Is it true Jeff_Bridges occupation Lane Chandl...   \n",
       "3  What is the pre-requisite of phase matter of G...   \n",
       "4  Which is the operating income for Qantas? <ext...   \n",
       "\n",
       "                                                   y  \n",
       "0  <extra_id_6> <extra_id_21> <extra_id_39> <extr...  \n",
       "1  <extra_id_6> <extra_id_39> <extra_id_19> <extr...  \n",
       "2  <extra_id_4> <extra_id_19> <extra_id_33> <extr...  \n",
       "3  <extra_id_6> <extra_id_39> <extra_id_19> <extr...  \n",
       "4  <extra_id_6> <extra_id_21> <extra_id_39> <extr...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Now we need to generate a T5 model for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import transformers\n",
    "# from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")\n",
    "        pprint(self.model.hf_device_map)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        outputs = self.model(\n",
    "            input_ids = input['input_ids'],\n",
    "            labels = input['labels'],\n",
    "            attention_mask = input['attention_mask'],\n",
    "            output_hidden_states = True,\n",
    "            output_attentions = True\n",
    "        )\n",
    "\n",
    "        return outputs.loss\n",
    "\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\") # Device_map splits the load over multiple GPUs, this seems to be quite new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class Train:\n",
    "    def __init__(self,data,data_val, model_name):\n",
    "        self.data=data\n",
    "        self.dev_data=data_val\n",
    "\n",
    "        self.tokenizer=T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model=Model(model_name)\n",
    "        # self.model.to(f'cuda:{self.model.device_ids[0]}')  \n",
    "           \n",
    "        # Modify lr?\n",
    "        self.optimizer=optim.AdamW(self.model.parameters(),lr=0.0015)\n",
    "        self.lr_scheduler=transformers. \\\n",
    "        get_polynomial_decay_schedule_with_warmup(self.optimizer, 5000, 30000,power=0.5)\n",
    "\n",
    "        self.iters=60000\n",
    "        self.print_every=100\n",
    "        self.eval_every=8000\n",
    "        # self.num_gpus=1\n",
    "        self.eval_bs=6\n",
    "        self.bs=5\n",
    "        self.back_propogate=10\n",
    "        \n",
    "        self.train()\n",
    "\n",
    "    def generate_batch(self):\n",
    "        output=random.sample(self.data,self.bs)\n",
    "        inp,label=[],[]\n",
    "        for dat in output:\n",
    "            inp.append(dat[0])\n",
    "            label.append(dat[1])\n",
    "\n",
    "        return inp,label\n",
    "\n",
    "    def preprocess_function(self,inputs, targets):\n",
    "        model_inputs=self.tokenizer(inputs, padding=True, \\\n",
    "                        return_tensors='pt',max_length=512, truncation=True)\n",
    "        labels=self.tokenizer(targets,padding=True,max_length=512, truncation=True)\n",
    "\n",
    "        if True:\n",
    "            labels[\"input_ids\"] = [\n",
    "            [(l if l != self.tokenizer.pad_token_id else -100) \\\n",
    "             for l in label] for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "        labels['input_ids']=torch.tensor(labels['input_ids'])\n",
    "        model_inputs[\"labels\"]=labels[\"input_ids\"].to(0)\n",
    "        model_inputs[\"input_ids\"]=model_inputs[\"input_ids\"].to(0)\n",
    "        model_inputs[\"attention_mask\"]=model_inputs[\"attention_mask\"].to(0)\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    def val(self,o):\n",
    "        print('Evaluating ...')\n",
    "        self.model.eval()\n",
    "        acc,bs,i=0,self.eval_bs,0\n",
    "        saver=[]\n",
    "\n",
    "        progress_bar = tqdm.auto.tqdm(range(len(self.dev_data)))\n",
    "        progress_bar.set_description(f\"Eval {o}\")\n",
    "           \n",
    "        while i<len(self.dev_data):\n",
    "            bs_=min(bs,len(self.dev_data)-i)\n",
    "            i+=bs_\n",
    "            inp,label=[],[]\n",
    "            for j in range(i-bs_,i):\n",
    "                inp.append(self.dev_data[j][0])\n",
    "                label.append(self.dev_data[j][1])\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "\n",
    "            input=self.preprocess_function(inp,label)\n",
    "\n",
    "            output=self.model.model.generate(input_ids=input['input_ids'],\n",
    "                      num_beams=10,attention_mask=input['attention_mask'], \\\n",
    "                        early_stopping=True, max_length=200,output_hidden_states=True,output_attentions=True)\n",
    "            \n",
    "            out=self.tokenizer.batch_decode(output,skip_special_tokens=False)\n",
    "\n",
    "            for k in range(len(out)):\n",
    "                #print(out[k].replace('<pad>','').replace('</s>','').strip())\n",
    "                a1=out[k].replace('<pad>','').replace('</s>','').replace('<unk>','').replace('<s>','').strip().replace(' ','')\n",
    "                a2=label[k].strip().replace(' ','')\n",
    "                #print(a1, '       ', a2)\n",
    "                saver.append({'input':inp[k],'gold':label[k].strip(),'generated':out[k].replace('<pad>',''). \\\n",
    "                      replace('</s>','').replace('<unk>','').replace('<s>','').strip()})\n",
    "                if a1==a2:\n",
    "                    acc+=1; #print('ttt')\n",
    "        \n",
    "        file=open('_dev_result'+str(o)+'.json','w')\n",
    "        json.dump(saver,file)\n",
    "        file.close()\n",
    "        return 100*acc/len(self.dev_data)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        scalar=0\n",
    "        for i in range(self.iters):\n",
    "            self.model.train()\n",
    "            inp,label=self.generate_batch()\n",
    "            input=self.preprocess_function(inp,label)\n",
    "            loss=self.model(input)\n",
    "\n",
    "            scalar+=loss.mean().item()\n",
    "            if(i+1)%self.print_every==0:\n",
    "                print('iteration={}, training loss={}'.format(i+1,scalar/self.print_every))\n",
    "                scalar=0\n",
    "            if(i + 1)%self.eval_every==0:\n",
    "                acc=self.val(i+1)\n",
    "                print('validation acc={}'.format(acc))\n",
    "\n",
    "                torch.save(self.model.state_dict(),\n",
    "                       join(lcquad2_dir,'checkpoints','_checkpoint'+str(i+1)+'.pth'))\n",
    "            \n",
    "            loss/=self.back_propogate\n",
    "            loss.mean().backward()\n",
    "            if (i+1)%self.back_propogate:\n",
    "                self.optimizer.step();\n",
    "                self.lr_scheduler.step();\n",
    "                self.optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n",
      "iteration=100, training loss=11.875922498703003\n",
      "iteration=200, training loss=8.44142150402069\n",
      "iteration=300, training loss=5.084807028770447\n",
      "iteration=400, training loss=3.4063505482673646\n",
      "iteration=500, training loss=2.803144739866257\n",
      "iteration=600, training loss=2.4220544290542603\n",
      "iteration=700, training loss=2.1986151552200317\n",
      "iteration=800, training loss=2.056194007396698\n",
      "iteration=900, training loss=1.897970929145813\n",
      "iteration=1000, training loss=1.7191280126571655\n",
      "iteration=1100, training loss=1.54655078291893\n",
      "iteration=1200, training loss=1.4155311930179595\n",
      "iteration=1300, training loss=1.2612022894620896\n",
      "iteration=1400, training loss=1.1871768152713775\n",
      "iteration=1500, training loss=1.090314125418663\n",
      "iteration=1600, training loss=1.0075640922784805\n",
      "iteration=1700, training loss=0.934908681511879\n",
      "iteration=1800, training loss=0.9371164274215699\n",
      "iteration=1900, training loss=0.8896639636158943\n",
      "iteration=2000, training loss=0.723104837834835\n",
      "iteration=2100, training loss=0.6737189081311226\n",
      "iteration=2200, training loss=0.6233870807290077\n",
      "iteration=2300, training loss=0.5522866070270538\n",
      "iteration=2400, training loss=0.5492084068059921\n",
      "iteration=2500, training loss=0.4928873234987259\n",
      "iteration=2600, training loss=0.42345416024327276\n",
      "iteration=2700, training loss=0.38217291310429574\n",
      "iteration=2800, training loss=0.38426791071891786\n",
      "iteration=2900, training loss=0.41703286692500113\n",
      "iteration=3000, training loss=0.2775304792076349\n",
      "iteration=3100, training loss=0.25392966620624063\n",
      "iteration=3200, training loss=0.2999724340438843\n",
      "iteration=3300, training loss=0.23746103465557097\n",
      "iteration=3400, training loss=0.19458136208355425\n",
      "iteration=3500, training loss=0.16557421158999205\n",
      "iteration=3600, training loss=0.16585317332297564\n",
      "iteration=3700, training loss=0.1757286322116852\n",
      "iteration=3800, training loss=0.15432174287736417\n",
      "iteration=3900, training loss=0.17154994308948518\n",
      "iteration=4000, training loss=0.16452369179576634\n",
      "iteration=4100, training loss=0.12697295539081097\n",
      "iteration=4200, training loss=0.1931509628146887\n",
      "iteration=4300, training loss=0.13461175125092267\n",
      "iteration=4400, training loss=0.0855103757418692\n",
      "iteration=4500, training loss=0.09380574369803071\n",
      "iteration=4600, training loss=0.10920683391392232\n",
      "iteration=4700, training loss=0.126729051489383\n",
      "iteration=4800, training loss=0.14546865116804839\n",
      "iteration=4900, training loss=0.14542265746742486\n",
      "iteration=5000, training loss=0.13326004460453988\n",
      "iteration=5100, training loss=0.15542057304643095\n",
      "iteration=5200, training loss=0.12460618239827453\n",
      "iteration=5300, training loss=0.11676960380747914\n",
      "iteration=5400, training loss=0.09371082762256265\n",
      "iteration=5500, training loss=0.09534357050433755\n",
      "iteration=5600, training loss=0.08355982334353029\n",
      "iteration=5700, training loss=0.07626636298373342\n",
      "iteration=5800, training loss=0.10079496186226607\n",
      "iteration=5900, training loss=0.10151297464035451\n",
      "iteration=6000, training loss=0.0709049553796649\n",
      "iteration=6100, training loss=0.053704642755910754\n",
      "iteration=6200, training loss=0.07175604653079062\n",
      "iteration=6300, training loss=0.10120860227383673\n",
      "iteration=6400, training loss=0.06741131232585758\n",
      "iteration=6500, training loss=0.05933096219552681\n",
      "iteration=6600, training loss=0.046512048956938085\n",
      "iteration=6700, training loss=0.03753996052779257\n",
      "iteration=6800, training loss=0.03513149530859664\n",
      "iteration=6900, training loss=0.04159883268177509\n",
      "iteration=7000, training loss=0.04076640353538096\n",
      "iteration=7100, training loss=0.13546489999163897\n",
      "iteration=7200, training loss=0.07015389484353364\n",
      "iteration=7300, training loss=0.061725471019744876\n",
      "iteration=7400, training loss=0.0839951589377597\n",
      "iteration=7500, training loss=0.0904736637044698\n",
      "iteration=7600, training loss=0.05721179633401334\n",
      "iteration=7700, training loss=0.05270835465751588\n",
      "iteration=7800, training loss=0.07239806706085801\n",
      "iteration=7900, training loss=0.06625552452169359\n",
      "iteration=8000, training loss=0.05041976151522249\n",
      "Evaluating ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval 8000: : 1335it [22:28,  1.01it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m total_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data)\n\u001b[1;32m      3\u001b[0m final_data, final_data_dev \u001b[39m=\u001b[39m data[:total_len\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m], data[total_len\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m:]\n\u001b[0;32m----> 4\u001b[0m trainer \u001b[39m=\u001b[39m Train(final_data, final_data_dev, \u001b[39m\"\u001b[39;49m\u001b[39mt5-small\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m, in \u001b[0;36mTrain.__init__\u001b[0;34m(self, data, data_val, model_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mback_propogate\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[0;32mIn[12], line 109\u001b[0m, in \u001b[0;36mTrain.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m     scalar\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m%\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_every\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[0;32m--> 109\u001b[0m     acc\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval(i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    110\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mvalidation acc=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(acc))\n\u001b[1;32m    112\u001b[0m     torch\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mstate_dict(),\n\u001b[1;32m    113\u001b[0m            join(lcquad2_dir,\u001b[39m'\u001b[39m\u001b[39mcheckpoints\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m_checkpoint\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[12], line 74\u001b[0m, in \u001b[0;36mTrain.val\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m     70\u001b[0m progress_bar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_function(inp,label)\n\u001b[0;32m---> 74\u001b[0m output\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     75\u001b[0m           num_beams\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,attention_mask\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m], \\\n\u001b[1;32m     76\u001b[0m             early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,output_attentions\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     78\u001b[0m out\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mbatch_decode(output,skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     80\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(out)):\n\u001b[1;32m     81\u001b[0m     \u001b[39m#print(out[k].replace('<pad>','').replace('</s>','').strip())\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/generation/utils.py:1524\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1523\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1524\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1525\u001b[0m         input_ids,\n\u001b[1;32m   1526\u001b[0m         beam_scorer,\n\u001b[1;32m   1527\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1528\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1529\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1530\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1531\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1532\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1533\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1534\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1535\u001b[0m     )\n\u001b[1;32m   1537\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1538\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/generation/utils.py:2810\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2806\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2808\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2810\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2811\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2812\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2813\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2814\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2815\u001b[0m )\n\u001b[1;32m   2817\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2818\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1716\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1713\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1715\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> 1716\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1717\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1718\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1719\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1720\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1721\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   1722\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1723\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1724\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1725\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1726\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1727\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1728\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1729\u001b[0m )\n\u001b[1;32m   1731\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1733\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1086\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m   1074\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1075\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     )\n\u001b[1;32m   1085\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1086\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1087\u001b[0m         hidden_states,\n\u001b[1;32m   1088\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1089\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   1090\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1091\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1092\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   1093\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1094\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1095\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1096\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1097\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1098\u001b[0m     )\n\u001b[1;32m   1100\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:753\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    750\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    752\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 753\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n\u001b[1;32m    755\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:342\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 342\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_norm(hidden_states)\n\u001b[1;32m    343\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDenseReluDense(forwarded_states)\n\u001b[1;32m    344\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(forwarded_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:257\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    254\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrsqrt(variance \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariance_epsilon)\n\u001b[1;32m    256\u001b[0m \u001b[39m# convert into half-precision if necessary\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39mdtype \u001b[39min\u001b[39;00m [torch\u001b[39m.\u001b[39mfloat16, torch\u001b[39m.\u001b[39mbfloat16]:\n\u001b[1;32m    258\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    260\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m*\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1256\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_is_full_backward_hook\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1254\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_full_backward_hook \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1257\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1258\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = df.values.tolist()\n",
    "total_len = len(data)\n",
    "final_data, final_data_dev = data[:total_len//10], data[total_len//10:]\n",
    "trainer = Train(final_data, final_data_dev, \"t5-small\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
