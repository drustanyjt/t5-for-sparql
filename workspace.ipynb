{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # To order device based on pci bus id\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" # As the name suggests these are the VISIBLE GPUS, you need to make use of them using cuda:0/1\n",
    "# Don't set the below variables, they are always relative\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = \"/$HOME/.cache/huggingface/datasets\" \n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"/$HOME/.cache/huggingface/hub\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF Spider\n",
    "Note that Hugging Face's spider dataset does not work with Picard T5 because it does not serialize the DB schemas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"spider\")\n",
    "ds_train = dataset[\"train\"] # To get the training rows\n",
    "pprint(ds_train[0:1].keys())\n",
    "pprint(ds_train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yale Spider\n",
    "The original Spider dataset was downloaded from their [homepage](https://yale-lily.github.io/spider).\n",
    "The 2 important files are described on their [github](https://github.com/taoyds/spider) as follows:\n",
    "1. **train.json/dev.json**\n",
    "    - `question`: the natural language question\n",
    "    - `question_toks`: the natural language question tokens\n",
    "    - `db_id`: the database id to which this question is addressed.\n",
    "    - `query`: the SQL query corresponding to the question.\n",
    "    - `query_toks`: the SQL query tokens corresponding to the question.\n",
    "    - `sql`: parsed results of this SQL query using process_sql.py. Please refer to parsed_sql_examples.sql in thepreprocess directory for the detailed documentation.\n",
    "2. **tables.json** contains the schema of all tables\n",
    "    - `db_id`: database id\n",
    "    - `table_names_original`: original table names stored in the database.\n",
    "    - `table_names`: cleaned and normalized table names. We make sure the table names are meaningful. [to be changed]\n",
    "    - `column_names_original`: original column names stored in the database. Each column looks like: [0, \"id\"]. 0 is the index of table names in table_names, which is city in this case. \"id\" is the column name.\n",
    "    - `column_names`: cleaned and normalized column names. We make sure the column names are meaningful. [to be changed]\n",
    "    - `column_types`: data type of each column\n",
    "    - `foreign_keys`: foreign keys in the database. [3, 8] means column indices in the column_names. These two columns are foreign keys of two different tables.\n",
    "    - `primary_keys`: primary keys in the database. Each number is the index of column_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_others.json', 'dev.json', 'README.txt', 'database', '.DS_Store', 'train_spider.json', 'tables.json', 'train_gold.sql', 'dev_gold.sql']\n",
      "\n",
      "---train_spider.json---\n",
      "dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql'])\n",
      "\n",
      "---tables.json---\n",
      "dict_keys(['column_names', 'column_names_original', 'column_types', 'db_id', 'foreign_keys', 'primary_keys', 'table_names', 'table_names_original'])\n",
      "\n",
      "---dev.json---\n",
      "dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql'])\n"
     ]
    }
   ],
   "source": [
    "spider_dir_path = \"./data/spider\"\n",
    "\n",
    "print(os.listdir(spider_dir_path))\n",
    "print()\n",
    "\n",
    "train_json_filename = \"train_spider.json\"\n",
    "print(f\"---{train_json_filename}---\")\n",
    "with open(os.path.join(spider_dir_path, train_json_filename)) as f:\n",
    "    train_json = json.load(f)\n",
    "print(train_json[0].keys())\n",
    "print()\n",
    "\n",
    "tables_json_filename = \"tables.json\"\n",
    "print(f\"---{tables_json_filename}---\")\n",
    "with open(os.path.join(spider_dir_path, tables_json_filename)) as f:\n",
    "    tables_json = json.load(f)\n",
    "print(tables_json[0].keys())\n",
    "print()\n",
    "\n",
    "dev_json_filename = \"dev.json\"\n",
    "print(f\"---{dev_json_filename}---\")\n",
    "with open(os.path.join(spider_dir_path, dev_json_filename)) as f:\n",
    "    dev_json = json.load(f)\n",
    "print(dev_json[0].keys())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables json is a list, so we load in memory as a dictionary (hashtable) for quicker access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of both data structures match: True\n"
     ]
    }
   ],
   "source": [
    "tables_dict_by_db = {}\n",
    "for table in tables_json:\n",
    "    tables_dict_by_db[table[\"db_id\"]] = table \n",
    "print(\"Length of both data structures match:\", len(tables_json) == len(tables_dict_by_db))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of one entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databse: movie_1\n",
      "---column_names---\n",
      "[-1, '*']|[0, 'movie id']|[0, 'title']|[0, 'year']|[0, 'director']|[1, 'reviewer id']|[1, 'name']|[2, 'reviewer id']|[2, 'movie id']|[2, 'rating stars']|[2, 'rating date']\n",
      "---column_names_original---\n",
      "[-1, '*']|[0, 'mID']|[0, 'title']|[0, 'year']|[0, 'director']|[1, 'rID']|[1, 'name']|[2, 'rID']|[2, 'mID']|[2, 'stars']|[2, 'ratingDate']\n",
      "---column_types---\n",
      "text|number|text|number|text|number|text|number|number|number|time\n",
      "---foreign_keys---\n",
      "[7, 5]|[8, 1]\n",
      "---primary_keys---\n",
      "1|5\n",
      "---table_names---\n",
      "movie|reviewer|rating\n",
      "---table_names_original---\n",
      "Movie|Reviewer|Rating\n"
     ]
    }
   ],
   "source": [
    "list(tables_dict_by_db.keys())[99] #movie_1\n",
    "movie_1 = tables_dict_by_db[\"movie_1\"]\n",
    "print(\"Databse:\", movie_1[\"db_id\"])\n",
    "for key in movie_1:\n",
    "    # Skip db_id since it is a string\n",
    "    if key == \"db_id\":\n",
    "        continue\n",
    "    print(f\"---{key}---\")\n",
    "    item_collated = \"|\".join(str(item) for item in movie_1[key])\n",
    "    print(item_collated)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try to serialize each database's schema in accordance with Tscholak (who in turn bases it of Shaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie_1 | movie : movie_id, title, year, director | reviewer : reviewer_id, name | rating : reviewer_id, movie_id, rating_stars, rating_date'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO find out how to describe fields\n",
    "delimiter = \" | \"\n",
    "def serialize_spider_db(db):\n",
    "\n",
    "    # First group column names by their table id\n",
    "    columns = db[\"column_names\"]\n",
    "    column_strings = {}\n",
    "    for column in columns:\n",
    "        table_idx = column[0]\n",
    "        if table_idx not in column_strings:\n",
    "            column_strings[table_idx] = [] \n",
    "        # Note that the white spaces in column names were replaced with underscores (arbitrarily I suppose)\n",
    "        column_strings[table_idx].append(column[1].replace(\" \",\"_\"))\n",
    "    \n",
    "    # Next combine table name with column names\n",
    "    tables = db[\"table_names\"]\n",
    "    table_strings = [db[\"db_id\"]]\n",
    "    for table_idx in range(len(tables)):\n",
    "        table_name = tables[table_idx]\n",
    "        columns_serialized = \", \".join(column_strings[table_idx])\n",
    "        table_serialized = table_name + \" : \" + columns_serialized\n",
    "        table_strings.append(table_serialized)\n",
    "    \n",
    "    # Lastly combine all serialized table names together with the db id\n",
    "    schema_serialized = delimiter.join(table_strings)\n",
    "    return schema_serialized\n",
    "\n",
    "serialize_spider_db(movie_1) # 'movie_1 | movie : movie id, title, year, director | reviewer : reviewer id, name | rating : reviewer id, movie id, rating stars, rating date'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will no create some utility functions that will help convert the spider dataset into a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input: How many heads of the departments are older than 56 ? | department_management | department : department_id, name, creation, ranking, budget_in_billions, num_employees | head : head_id, name, born_state, age | management : department_id, head_id, temporary_acting\n",
      "Example output: SELECT count(*) FROM head WHERE age  >  56\n",
      "Example training data:\n",
      "{'input': ['How many heads of the departments are older than 56 ? | '\n",
      "           'department_management | department : department_id, name, '\n",
      "           'creation, ranking, budget_in_billions, num_employees | head : '\n",
      "           'head_id, name, born_state, age | management : department_id, '\n",
      "           'head_id, temporary_acting'],\n",
      " 'output': ['SELECT count(*) FROM head WHERE age  >  56']}\n"
     ]
    }
   ],
   "source": [
    "def test_2_preprocessed(test_case):\n",
    "    db_id = test_case['db_id']\n",
    "    db_schema = tables_dict_by_db[db_id]\n",
    "    db_schema_serialized = serialize_spider_db(db_schema)\n",
    "    question = test_case[\"question\"]\n",
    "    return delimiter.join([question, db_schema_serialized])\n",
    "\n",
    "def test_2_answer(test_case):\n",
    "    return test_case['query']\n",
    "\n",
    "def convert_to_training(test_cases : list):\n",
    "    # This takes in a list and outputs two lists\n",
    "    if not isinstance(test_cases, list):\n",
    "        test_cases = [test_cases]\n",
    "\n",
    "    training_set = {\n",
    "        \"input\" : [],\n",
    "        \"output\": [],\n",
    "    }\n",
    "\n",
    "    def helper(test_case):\n",
    "        training_set[\"input\"].append(test_2_preprocessed(test_case))\n",
    "        training_set[\"output\"].append(test_2_answer(test_case))\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        helper(test_case)\n",
    "\n",
    "    return training_set\n",
    "\n",
    "\n",
    "train_example = train_json[0]\n",
    "print(\"Example input:\", test_2_preprocessed(train_example))\n",
    "print(\"Example output:\", test_2_answer(train_example))\n",
    "print(\"Example training data:\")\n",
    "pprint(convert_to_training(train_example))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get the final variables for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How many heads of the departments are older than 56 ? | '\n",
      " 'department_management | department : department_id, name, creation, ranking, '\n",
      " 'budget_in_billions, num_employees | head : head_id, name, born_state, age | '\n",
      " 'management : department_id, head_id, temporary_acting',\n",
      " 'List the name, born state and age of the heads of departments ordered by '\n",
      " 'age. | department_management | department : department_id, name, creation, '\n",
      " 'ranking, budget_in_billions, num_employees | head : head_id, name, '\n",
      " 'born_state, age | management : department_id, head_id, temporary_acting']\n",
      "['SELECT count(*) FROM head WHERE age  >  56',\n",
      " 'SELECT name ,  born_state ,  age FROM head ORDER BY age']\n"
     ]
    }
   ],
   "source": [
    "training_set = convert_to_training(train_json)\n",
    "training_inputs = training_set['input']\n",
    "training_outputs = training_set['output']\n",
    "pprint(training_inputs[:2])\n",
    "pprint(training_outputs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['in', 'out'],\n",
       "        num_rows: 6300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['in', 'out'],\n",
       "        num_rows: 700\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "ins = [{'in':x[0],'out':x[1]} for x in zip(training_inputs, training_outputs)]\n",
    "dataset = Dataset.from_list(ins)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Loading model into RAM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picard + T5\n",
    "From Tscholak"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch \n",
    "torch.cuda.set_device(2)\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tscholak/cxmefzzi\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"tscholak/cxmefzzi\").to(\"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "Try to follow the HF tutorial with tscholak"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing With a Tokenizer\n",
    "The example sentence was taken from the HF website."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw_inputs = [\n",
    "    \"How many singers do we have? | concert_singer | stadium : stadium_id, location, name, capacity, highest, lowest, average | singer : singer_id, name, country, song_name, song_release_year, age, is_male | concert : concert_id, concert_name, theme, stadium_id, year | singer_in_concert : concert_id, singer_id\",\n",
    "]\n",
    "# raw_inputs = map(test_2_preprocessed, train_json[0:50])\n",
    "inputs = tokenizer(list(raw_inputs), padding=True, return_tensors=\"pt\")\n",
    "print(inputs[\"input_ids\"].size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Inference\n",
    "The model generates the desired response. It seems that the SQL tokens are generally lowercase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs = model.generate(inputs.input_ids, max_new_tokens=512)\n",
    "# outputs = model.generate(**inputs, decoder_input_ids=decoder_inputs.input_ids, max_new_tokens=1024)\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-Small\n",
    "For practice we will try to re-train T5 using the spider dataset. Picard based itself of the [T5ForConditionalGeneration](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5ForConditionalGeneration).\n",
    "\n",
    "I roughly followed the [Hugging Face fine-tuning pre-trained models guide](https://huggingface.co/docs/transformers/training#finetune-a-pretrained-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shared': 0,\n",
       " 'decoder.embed_tokens': 0,\n",
       " 'encoder.embed_tokens': 0,\n",
       " 'lm_head': 0,\n",
       " 'encoder.final_layer_norm': 1,\n",
       " 'encoder.dropout': 1,\n",
       " 'decoder.block.0': 1,\n",
       " 'decoder.block.1': 2,\n",
       " 'decoder.block.2': 2,\n",
       " 'decoder.block.3': 2,\n",
       " 'decoder.block.4': 2,\n",
       " 'decoder.block.5': 2,\n",
       " 'decoder.final_layer_norm': 2,\n",
       " 'decoder.dropout': 2,\n",
       " 'encoder.block': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration, T5Config\n",
    "from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\") # Device_map splits the load over multiple GPUs, this seems to be quite new\n",
    "model.hf_device_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset class supports chaining use of maps (like a monad). We tokenize the raw input strings and split into a train and test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that dataset returns data as lists, you have to [explicitly](https://discuss.huggingface.co/t/dataset-map-return-only-list-instead-torch-tensors/15767/2) set the format to tensors. This function returns nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e35e78a2c945e099e502e432d0f5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4c1b75cfe443509419f02e7c134b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 700\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_wrapper(examples):\n",
    "    return tokenizer(examples[\"in\"], text_target=examples[\"out\"], padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "tokenized_datasets = dataset.map(tokenize_wrapper, batched=True).remove_columns([\"in\", \"out\"])\n",
    "\n",
    "tokenized_datasets.set_format(\"pt\", columns=[\"input_ids\", \"attention_mask\" ,\"labels\"], output_all_columns=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle()#.select(range(10))\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle()#.select(range(100))\n",
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Single GPU\n",
    "Note that the model is located across the vram of multiple GPUs, but I'm not sure if the training itself leverages that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# training_args = TrainingArguments(output_dir = \"checkpoints/text2sql-t5small-spider\")\n",
    "\n",
    "# import numpy as np\n",
    "# import evaluate\n",
    "# # Make the Metric\n",
    "# # dont forget to pip uninstall once we change to a proper metric, you should probably leave sk learn in\n",
    "\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# def compute_metric(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# # Make the Trainer\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=small_train_dataset,\n",
    "#     eval_dataset=small_test_dataset,\n",
    "#     compute_metrics=compute_metric,\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerator for Training\n",
    "\n",
    "To work with Accelerator, we have to use the native pyTorch APIs. HuggingFace provides a guide for using [native PyTorch](https://huggingface.co/docs/transformers/training#train-in-native-pytorch), and a reference for usinge these to [accelerate](https://huggingface.co/docs/transformers/accelerate#prepare-to-accelerate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86950d1b8a6047fb8fcc5869defc2509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "test_dataloader = DataLoader(small_test_dataset, batch_size=8)\n",
    "\n",
    "accelerator = Accelerator(project_dir=f\"checkpoints/text2sql-{model_name}-spider-accelerate\")\n",
    "type(small_train_dataset[\"input_ids\"][0:2])\n",
    "# Optimizer and Learning Rate shceduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "train_dataloader, test_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, test_dataloader, model, optimizer\n",
    ")\n",
    "# Logging maybe try WandB?\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # batch = {k: v.to(0) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "\n",
    "# metric = evaluate.load(\"accuracy\") # Idk what is an accuracy metric\n",
    "# model.eval()\n",
    "# for batch in test_dataloader:\n",
    "#     batch = {k: v.to(0) for k, v in batch.items()}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**batch)\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "# metric.compute() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing\n",
    "\n",
    "This is the example given on the Picard HF website. The expected output is `SELECT COUNT (*) FROM singer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 102])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_inputs = [\n",
    "    \"How many singers do we have? | concert_singer | stadium : stadium_id, location, name, capacity, highest, lowest, average | singer : singer_id, name, country, song_name, song_release_year, age, is_male | concert : concert_id, concert_name, theme, stadium_id, year | singer_in_concert : concert_id, singer_id\",\n",
    "]\n",
    "# raw_inputs = map(test_2_preprocessed, train_json[0:50])\n",
    "inputs = tokenizer(list(raw_inputs), padding=True, return_tensors=\"pt\")\n",
    "print(inputs[\"input_ids\"].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['| concert_singer | stadium_id, location, name, capacity, highest, lowest, lowest, highest, lowest, average | singer : singer_id, name, country, song_name, song_release_year, age, is_male | concert : concert_id, song_release_year, age, is_male | concert : concert_id, singer_in_concert : concert_id, singer_id, year | singer_in_concert']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.to(0)\n",
    "outputs = model.generate(inputs.input_ids, max_new_tokens=512)\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inputs = tokenizer(training_inputs[0:100], text_target=training_outputs[0:100], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# model_inputs.keys()\n",
    "# # training\n",
    "# inputs = tokenizer(training_inputs[0:100], padding=True, return_tensors=\"pt\")\n",
    "# labels = tokenizer(training_outputs[0:100], padding=True, return_tensors=\"pt\")\n",
    "# outputs = model(input_ids=inputs.input_ids, labels=labels.input_ids)\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
