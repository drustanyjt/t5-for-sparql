{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF Spider\n",
    "Note that Hugging Face's spider dataset does not work with Picard T5 because it does not serialize the DB schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset spider (/home/yjunteng/.cache/huggingface/datasets/spider/spider/1.0.0/4e5143d825a3895451569c8b9b55432b91a4bc2d04d390376c950837f4680daa)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e83955f8c441378dae4814327731de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"spider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = dataset[\"train\"] # To get the training rows\n",
    "pprint(ds_train[0:1].keys())\n",
    "pprint(ds_train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yale Spider\n",
    "The original Spider dataset was downloaded from their [homepage](https://yale-lily.github.io/spider).\n",
    "The 2 important files are described on their [github](https://github.com/taoyds/spider) as follows:\n",
    "1. **train.json/dev.json**\n",
    "    - `question`: the natural language question\n",
    "    - `question_toks`: the natural language question tokens\n",
    "    - `db_id`: the database id to which this question is addressed.\n",
    "    - `query`: the SQL query corresponding to the question.\n",
    "    - `query_toks`: the SQL query tokens corresponding to the question.\n",
    "    - `sql`: parsed results of this SQL query using process_sql.py. Please refer to parsed_sql_examples.sql in thepreprocess directory for the detailed documentation.\n",
    "2. **tables.json** contains the schema of all tables\n",
    "    - `db_id`: database id\n",
    "    - `table_names_original`: original table names stored in the database.\n",
    "    - `table_names`: cleaned and normalized table names. We make sure the table names are meaningful. [to be changed]\n",
    "    - `column_names_original`: original column names stored in the database. Each column looks like: [0, \"id\"]. 0 is the index of table names in table_names, which is city in this case. \"id\" is the column name.\n",
    "    - `column_names`: cleaned and normalized column names. We make sure the column names are meaningful. [to be changed]\n",
    "    - `column_types`: data type of each column\n",
    "    - `foreign_keys`: foreign keys in the database. [3, 8] means column indices in the column_names. These two columns are foreign keys of two different tables.\n",
    "    - `primary_keys`: primary keys in the database. Each number is the index of column_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_others.json', 'dev.json', 'README.txt', 'database', '.DS_Store', 'train_spider.json', 'tables.json', 'train_gold.sql', 'dev_gold.sql']\n",
      "\n",
      "---train_spider.json---\n",
      "dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql'])\n",
      "\n",
      "---tables.json---\n",
      "dict_keys(['column_names', 'column_names_original', 'column_types', 'db_id', 'foreign_keys', 'primary_keys', 'table_names', 'table_names_original'])\n",
      "\n",
      "---dev.json---\n",
      "dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql'])\n"
     ]
    }
   ],
   "source": [
    "spider_dir_path = \"./data/spider\"\n",
    "\n",
    "print(os.listdir(spider_dir_path))\n",
    "print()\n",
    "\n",
    "train_json_filename = \"train_spider.json\"\n",
    "print(f\"---{train_json_filename}---\")\n",
    "with open(os.path.join(spider_dir_path, train_json_filename)) as f:\n",
    "    train_json = json.load(f)\n",
    "print(train_json[0].keys())\n",
    "print()\n",
    "\n",
    "tables_json_filename = \"tables.json\"\n",
    "print(f\"---{tables_json_filename}---\")\n",
    "with open(os.path.join(spider_dir_path, tables_json_filename)) as f:\n",
    "    tables_json = json.load(f)\n",
    "print(tables_json[0].keys())\n",
    "print()\n",
    "\n",
    "dev_json_filename = \"dev.json\"\n",
    "print(f\"---{dev_json_filename}---\")\n",
    "with open(os.path.join(spider_dir_path, dev_json_filename)) as f:\n",
    "    dev_json = json.load(f)\n",
    "print(dev_json[0].keys())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables json is a list, so we load in memory as a dictionary (hashtable) for quicker access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of both data structures match: True\n"
     ]
    }
   ],
   "source": [
    "tables_dict_by_db = {}\n",
    "for table in tables_json:\n",
    "    tables_dict_by_db[table[\"db_id\"]] = table \n",
    "print(\"Length of both data structures match:\", len(tables_json) == len(tables_dict_by_db))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of one entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databse: movie_1\n",
      "---column_names---\n",
      "[-1, '*']|[0, 'movie id']|[0, 'title']|[0, 'year']|[0, 'director']|[1, 'reviewer id']|[1, 'name']|[2, 'reviewer id']|[2, 'movie id']|[2, 'rating stars']|[2, 'rating date']\n",
      "---column_names_original---\n",
      "[-1, '*']|[0, 'mID']|[0, 'title']|[0, 'year']|[0, 'director']|[1, 'rID']|[1, 'name']|[2, 'rID']|[2, 'mID']|[2, 'stars']|[2, 'ratingDate']\n",
      "---column_types---\n",
      "text|number|text|number|text|number|text|number|number|number|time\n",
      "---foreign_keys---\n",
      "[7, 5]|[8, 1]\n",
      "---primary_keys---\n",
      "1|5\n",
      "---table_names---\n",
      "movie|reviewer|rating\n",
      "---table_names_original---\n",
      "Movie|Reviewer|Rating\n"
     ]
    }
   ],
   "source": [
    "list(tables_dict_by_db.keys())[99] #movie_1\n",
    "movie_1 = tables_dict_by_db[\"movie_1\"]\n",
    "print(\"Databse:\", movie_1[\"db_id\"])\n",
    "for key in movie_1:\n",
    "    # Skip db_id since it is a string\n",
    "    if key == \"db_id\":\n",
    "        continue\n",
    "    print(f\"---{key}---\")\n",
    "    item_collated = \"|\".join(str(item) for item in movie_1[key])\n",
    "    print(item_collated)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try to serialize each database's schema in accordance with Tscholak (who in turn bases it of Shaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie_1 | movie : movie_id, title, year, director | reviewer : reviewer_id, name | rating : reviewer_id, movie_id, rating_stars, rating_date'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO find out how to describe fields\n",
    "delimiter = \" | \"\n",
    "def serialize_spider_db(db):\n",
    "\n",
    "    # First group column names by their table id\n",
    "    columns = db[\"column_names\"]\n",
    "    column_strings = {}\n",
    "    for column in columns:\n",
    "        table_idx = column[0]\n",
    "        if table_idx not in column_strings:\n",
    "            column_strings[table_idx] = [] \n",
    "        # Note that the white spaces in column names were replaced with underscores (arbitrarily I suppose)\n",
    "        column_strings[table_idx].append(column[1].replace(\" \",\"_\"))\n",
    "    \n",
    "    # Next combine table name with column names\n",
    "    tables = db[\"table_names\"]\n",
    "    table_strings = [db[\"db_id\"]]\n",
    "    for table_idx in range(len(tables)):\n",
    "        table_name = tables[table_idx]\n",
    "        columns_serialized = \", \".join(column_strings[table_idx])\n",
    "        table_serialized = table_name + \" : \" + columns_serialized\n",
    "        table_strings.append(table_serialized)\n",
    "    \n",
    "    # Lastly combine all serialized table names together with the db id\n",
    "    schema_serialized = delimiter.join(table_strings)\n",
    "    return schema_serialized\n",
    "\n",
    "serialize_spider_db(movie_1) # 'movie_1 | movie : movie id, title, year, director | reviewer : reviewer id, name | rating : reviewer id, movie id, rating stars, rating date'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will no create some utility functions that will help convert the spider dataset into a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input: How many heads of the departments are older than 56 ? | department_management | department : department_id, name, creation, ranking, budget_in_billions, num_employees | head : head_id, name, born_state, age | management : department_id, head_id, temporary_acting\n",
      "Example output: SELECT count(*) FROM head WHERE age  >  56\n",
      "Example training data:\n",
      "{'input': ['How many heads of the departments are older than 56 ? | '\n",
      "           'department_management | department : department_id, name, '\n",
      "           'creation, ranking, budget_in_billions, num_employees | head : '\n",
      "           'head_id, name, born_state, age | management : department_id, '\n",
      "           'head_id, temporary_acting'],\n",
      " 'output': ['SELECT count(*) FROM head WHERE age  >  56']}\n"
     ]
    }
   ],
   "source": [
    "def test_2_preprocessed(test_case):\n",
    "    db_id = test_case['db_id']\n",
    "    db_schema = tables_dict_by_db[db_id]\n",
    "    db_schema_serialized = serialize_spider_db(db_schema)\n",
    "    question = test_case[\"question\"]\n",
    "    return delimiter.join([question, db_schema_serialized])\n",
    "\n",
    "def test_2_answer(test_case):\n",
    "    return test_case['query']\n",
    "\n",
    "def convert_to_training(test_cases : list):\n",
    "    # This takes in a list and outputs two lists\n",
    "    if not isinstance(test_cases, list):\n",
    "        test_cases = [test_cases]\n",
    "\n",
    "    training_set = {\n",
    "        \"input\" : [],\n",
    "        \"output\": [],\n",
    "    }\n",
    "\n",
    "    def helper(test_case):\n",
    "        training_set[\"input\"].append(test_2_preprocessed(test_case))\n",
    "        training_set[\"output\"].append(test_2_answer(test_case))\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        helper(test_case)\n",
    "\n",
    "    return training_set\n",
    "\n",
    "\n",
    "train_example = train_json[0]\n",
    "print(\"Example input:\", test_2_preprocessed(train_example))\n",
    "print(\"Example output:\", test_2_answer(train_example))\n",
    "print(\"Example training data:\")\n",
    "pprint(convert_to_training(train_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How many heads of the departments are older than 56 ? | '\n",
      " 'department_management | department : department_id, name, creation, ranking, '\n",
      " 'budget_in_billions, num_employees | head : head_id, name, born_state, age | '\n",
      " 'management : department_id, head_id, temporary_acting',\n",
      " 'List the name, born state and age of the heads of departments ordered by '\n",
      " 'age. | department_management | department : department_id, name, creation, '\n",
      " 'ranking, budget_in_billions, num_employees | head : head_id, name, '\n",
      " 'born_state, age | management : department_id, head_id, temporary_acting']\n",
      "['SELECT count(*) FROM head WHERE age  >  56',\n",
      " 'SELECT name ,  born_state ,  age FROM head ORDER BY age']\n"
     ]
    }
   ],
   "source": [
    "training_set = convert_to_training(train_json)\n",
    "training_inputs = training_set['input']\n",
    "training_outputs = training_set['output']\n",
    "pprint(training_inputs[:2])\n",
    "pprint(training_outputs[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Loading model into RAM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picard + T5\n",
    "From Tscholak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tscholak/cxmefzzi\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"tscholak/cxmefzzi\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "Try to follow the HF tutorial with tscholak"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing With a Tokenizer\n",
    "The example sentence was taken from the HF website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 102])\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"How many singers do we have? | concert_singer | stadium : stadium_id, location, name, capacity, highest, lowest, average | singer : singer_id, name, country, song_name, song_release_year, age, is_male | concert : concert_id, concert_name, theme, stadium_id, year | singer_in_concert : concert_id, singer_id\",\n",
    "]\n",
    "# raw_inputs = map(test_2_preprocessed, train_json[0:10])\n",
    "inputs = tokenizer(list(raw_inputs), padding=True, return_tensors=\"pt\")\n",
    "print(inputs[\"input_ids\"].size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Inference\n",
    "The model generates the desired response. It seems that the SQL tokens are generally lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['concert_singer | select count(*) from singer']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(inputs.input_ids, max_new_tokens=512)\n",
    "# outputs = model.generate(**inputs, decoder_input_ids=decoder_inputs.input_ids, max_new_tokens=1024)\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-Small\n",
    "For practice we will try to re-train T5 using the spider dataset. Picard based itself of the [T5ForConditionalGeneration](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5ForConditionalGeneration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenzier = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(3)\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "inputs = tokenizer(training_inputs, padding=True, return_tensors=\"pt\")\n",
    "labels = tokenizer(training_outputs, padding=True, return_tensors=\"pt\")\n",
    "outputs = model(input_ids=inputs.input_ids, labels=labels.input_ids)\n",
    "loss = outpus.loss\n",
    "logits = outputs.logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
