{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # To order device based on pci bus id\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" # As the name suggests these are the VISIBLE GPUS, you need to make use of them using cuda:0/1\n",
    "# Don't set the below variables, they are always relative\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = \"/$HOME/.cache/huggingface/datasets\" \n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"/$HOME/.cache/huggingface/hub\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF Spider\n",
    "Note that Hugging Face's spider dataset does not work with Picard T5 because it does not serialize the DB schemas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"spider\")\n",
    "ds_train = dataset[\"train\"] # To get the training rows\n",
    "pprint(ds_train[0:1].keys())\n",
    "pprint(ds_train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yale Spider\n",
    "The original Spider dataset was downloaded from their [homepage](https://yale-lily.github.io/spider).\n",
    "The 2 important files are described on their [github](https://github.com/taoyds/spider) as follows:\n",
    "1. **train.json/dev.json**\n",
    "    - `question`: the natural language question\n",
    "    - `question_toks`: the natural language question tokens\n",
    "    - `db_id`: the database id to which this question is addressed.\n",
    "    - `query`: the SQL query corresponding to the question.\n",
    "    - `query_toks`: the SQL query tokens corresponding to the question.\n",
    "    - `sql`: parsed results of this SQL query using process_sql.py. Please refer to parsed_sql_examples.sql in thepreprocess directory for the detailed documentation.\n",
    "2. **tables.json** contains the schema of all tables\n",
    "    - `db_id`: database id\n",
    "    - `table_names_original`: original table names stored in the database.\n",
    "    - `table_names`: cleaned and normalized table names. We make sure the table names are meaningful. [to be changed]\n",
    "    - `column_names_original`: original column names stored in the database. Each column looks like: [0, \"id\"]. 0 is the index of table names in table_names, which is city in this case. \"id\" is the column name.\n",
    "    - `column_names`: cleaned and normalized column names. We make sure the column names are meaningful. [to be changed]\n",
    "    - `column_types`: data type of each column\n",
    "    - `foreign_keys`: foreign keys in the database. [3, 8] means column indices in the column_names. These two columns are foreign keys of two different tables.\n",
    "    - `primary_keys`: primary keys in the database. Each number is the index of column_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_others.json', 'dev.json', 'README.txt', 'database', '.DS_Store', 'train_spider.json', 'tables.json', 'train_gold.sql', 'dev_gold.sql']\n",
      "\n",
      "---train_spider.json---\n",
      "dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql'])\n",
      "\n",
      "---tables.json---\n",
      "dict_keys(['column_names', 'column_names_original', 'column_types', 'db_id', 'foreign_keys', 'primary_keys', 'table_names', 'table_names_original'])\n",
      "\n",
      "---dev.json---\n",
      "dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql'])\n"
     ]
    }
   ],
   "source": [
    "spider_dir_path = \"./data/spider\"\n",
    "\n",
    "print(os.listdir(spider_dir_path))\n",
    "print()\n",
    "\n",
    "train_json_filename = \"train_spider.json\"\n",
    "print(f\"---{train_json_filename}---\")\n",
    "with open(os.path.join(spider_dir_path, train_json_filename)) as f:\n",
    "    train_json = json.load(f)\n",
    "print(train_json[0].keys())\n",
    "print()\n",
    "\n",
    "tables_json_filename = \"tables.json\"\n",
    "print(f\"---{tables_json_filename}---\")\n",
    "with open(os.path.join(spider_dir_path, tables_json_filename)) as f:\n",
    "    tables_json = json.load(f)\n",
    "print(tables_json[0].keys())\n",
    "print()\n",
    "\n",
    "dev_json_filename = \"dev.json\"\n",
    "print(f\"---{dev_json_filename}---\")\n",
    "with open(os.path.join(spider_dir_path, dev_json_filename)) as f:\n",
    "    dev_json = json.load(f)\n",
    "print(dev_json[0].keys())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables json is a list, so we load in memory as a dictionary (hashtable) for quicker access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of both data structures match: True\n"
     ]
    }
   ],
   "source": [
    "tables_dict_by_db = {}\n",
    "for table in tables_json:\n",
    "    tables_dict_by_db[table[\"db_id\"]] = table \n",
    "print(\"Length of both data structures match:\", len(tables_json) == len(tables_dict_by_db))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of one entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databse: movie_1\n",
      "---column_names---\n",
      "[-1, '*']|[0, 'movie id']|[0, 'title']|[0, 'year']|[0, 'director']|[1, 'reviewer id']|[1, 'name']|[2, 'reviewer id']|[2, 'movie id']|[2, 'rating stars']|[2, 'rating date']\n",
      "---column_names_original---\n",
      "[-1, '*']|[0, 'mID']|[0, 'title']|[0, 'year']|[0, 'director']|[1, 'rID']|[1, 'name']|[2, 'rID']|[2, 'mID']|[2, 'stars']|[2, 'ratingDate']\n",
      "---column_types---\n",
      "text|number|text|number|text|number|text|number|number|number|time\n",
      "---foreign_keys---\n",
      "[7, 5]|[8, 1]\n",
      "---primary_keys---\n",
      "1|5\n",
      "---table_names---\n",
      "movie|reviewer|rating\n",
      "---table_names_original---\n",
      "Movie|Reviewer|Rating\n"
     ]
    }
   ],
   "source": [
    "list(tables_dict_by_db.keys())[99] #movie_1\n",
    "movie_1 = tables_dict_by_db[\"movie_1\"]\n",
    "print(\"Databse:\", movie_1[\"db_id\"])\n",
    "for key in movie_1:\n",
    "    # Skip db_id since it is a string\n",
    "    if key == \"db_id\":\n",
    "        continue\n",
    "    print(f\"---{key}---\")\n",
    "    item_collated = \"|\".join(str(item) for item in movie_1[key])\n",
    "    print(item_collated)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try to serialize each database's schema in accordance with Tscholak (who in turn bases it of Shaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie_1 | movie : movie_id, title, year, director | reviewer : reviewer_id, name | rating : reviewer_id, movie_id, rating_stars, rating_date'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO find out how to describe fields\n",
    "delimiter = \" | \"\n",
    "def serialize_spider_db(db):\n",
    "\n",
    "    # First group column names by their table id\n",
    "    columns = db[\"column_names\"]\n",
    "    column_strings = {}\n",
    "    for column in columns:\n",
    "        table_idx = column[0]\n",
    "        if table_idx not in column_strings:\n",
    "            column_strings[table_idx] = [] \n",
    "        # Note that the white spaces in column names were replaced with underscores (arbitrarily I suppose)\n",
    "        column_strings[table_idx].append(column[1].replace(\" \",\"_\"))\n",
    "    \n",
    "    # Next combine table name with column names\n",
    "    tables = db[\"table_names\"]\n",
    "    table_strings = [db[\"db_id\"]]\n",
    "    for table_idx in range(len(tables)):\n",
    "        table_name = tables[table_idx]\n",
    "        columns_serialized = \", \".join(column_strings[table_idx])\n",
    "        table_serialized = table_name + \" : \" + columns_serialized\n",
    "        table_strings.append(table_serialized)\n",
    "    \n",
    "    # Lastly combine all serialized table names together with the db id\n",
    "    schema_serialized = delimiter.join(table_strings)\n",
    "    return schema_serialized\n",
    "\n",
    "serialize_spider_db(movie_1) # 'movie_1 | movie : movie id, title, year, director | reviewer : reviewer id, name | rating : reviewer id, movie id, rating stars, rating date'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will no create some utility functions that will help convert the spider dataset into a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input: How many heads of the departments are older than 56 ? | department_management | department : department_id, name, creation, ranking, budget_in_billions, num_employees | head : head_id, name, born_state, age | management : department_id, head_id, temporary_acting\n",
      "Example output: SELECT count(*) FROM head WHERE age  >  56\n",
      "Example training data:\n",
      "{'input': ['How many heads of the departments are older than 56 ? | '\n",
      "           'department_management | department : department_id, name, '\n",
      "           'creation, ranking, budget_in_billions, num_employees | head : '\n",
      "           'head_id, name, born_state, age | management : department_id, '\n",
      "           'head_id, temporary_acting'],\n",
      " 'output': ['SELECT count(*) FROM head WHERE age  >  56']}\n"
     ]
    }
   ],
   "source": [
    "def test_2_preprocessed(test_case):\n",
    "    db_id = test_case['db_id']\n",
    "    db_schema = tables_dict_by_db[db_id]\n",
    "    db_schema_serialized = serialize_spider_db(db_schema)\n",
    "    question = test_case[\"question\"]\n",
    "    return delimiter.join([question, db_schema_serialized])\n",
    "\n",
    "def test_2_answer(test_case):\n",
    "    return test_case['query']\n",
    "\n",
    "def convert_to_training(test_cases : list):\n",
    "    # This takes in a list and outputs two lists\n",
    "    if not isinstance(test_cases, list):\n",
    "        test_cases = [test_cases]\n",
    "\n",
    "    training_set = {\n",
    "        \"input\" : [],\n",
    "        \"output\": [],\n",
    "    }\n",
    "\n",
    "    def helper(test_case):\n",
    "        training_set[\"input\"].append(test_2_preprocessed(test_case))\n",
    "        training_set[\"output\"].append(test_2_answer(test_case))\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        helper(test_case)\n",
    "\n",
    "    return training_set\n",
    "\n",
    "\n",
    "train_example = train_json[0]\n",
    "print(\"Example input:\", test_2_preprocessed(train_example))\n",
    "print(\"Example output:\", test_2_answer(train_example))\n",
    "print(\"Example training data:\")\n",
    "pprint(convert_to_training(train_example))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get the final variables for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How many heads of the departments are older than 56 ? | '\n",
      " 'department_management | department : department_id, name, creation, ranking, '\n",
      " 'budget_in_billions, num_employees | head : head_id, name, born_state, age | '\n",
      " 'management : department_id, head_id, temporary_acting',\n",
      " 'List the name, born state and age of the heads of departments ordered by '\n",
      " 'age. | department_management | department : department_id, name, creation, '\n",
      " 'ranking, budget_in_billions, num_employees | head : head_id, name, '\n",
      " 'born_state, age | management : department_id, head_id, temporary_acting']\n",
      "['SELECT count(*) FROM head WHERE age  >  56',\n",
      " 'SELECT name ,  born_state ,  age FROM head ORDER BY age']\n"
     ]
    }
   ],
   "source": [
    "training_set = convert_to_training(train_json)\n",
    "training_inputs = training_set['input']\n",
    "training_outputs = training_set['output']\n",
    "pprint(training_inputs[:2])\n",
    "pprint(training_outputs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['in', 'out'],\n",
       "        num_rows: 6300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['in', 'out'],\n",
       "        num_rows: 700\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "ins = [{'in':x[0],'out':x[1]} for x in zip(training_inputs, training_outputs)]\n",
    "dataset = Dataset.from_list(ins)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Loading model into RAM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picard + T5\n",
    "From Tscholak"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch \n",
    "torch.cuda.set_device(2)\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tscholak/cxmefzzi\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"tscholak/cxmefzzi\").to(\"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "Try to follow the HF tutorial with tscholak"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing With a Tokenizer\n",
    "The example sentence was taken from the HF website."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw_inputs = [\n",
    "    \"How many singers do we have? | concert_singer | stadium : stadium_id, location, name, capacity, highest, lowest, average | singer : singer_id, name, country, song_name, song_release_year, age, is_male | concert : concert_id, concert_name, theme, stadium_id, year | singer_in_concert : concert_id, singer_id\",\n",
    "]\n",
    "# raw_inputs = map(test_2_preprocessed, train_json[0:50])\n",
    "inputs = tokenizer(list(raw_inputs), padding=True, return_tensors=\"pt\")\n",
    "print(inputs[\"input_ids\"].size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Inference\n",
    "The model generates the desired response. It seems that the SQL tokens are generally lowercase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs = model.generate(inputs.input_ids, max_new_tokens=512)\n",
    "# outputs = model.generate(**inputs, decoder_input_ids=decoder_inputs.input_ids, max_new_tokens=1024)\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-Small\n",
    "For practice we will try to re-train T5 using the spider dataset. Picard based itself of the [T5ForConditionalGeneration](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5ForConditionalGeneration).\n",
    "\n",
    "I roughly followed the [Hugging Face fine-tuning pre-trained models guide](https://huggingface.co/docs/transformers/training#finetune-a-pretrained-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a415020b3d49bdb8816f2610a80c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d2ae2e0bdf4ae4ad3d04876a2d2662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a760170d544739a25d602b732c2510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjunteng/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fda563eb12a4de1a61e1231c8aa7c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b85abd31d640e7b3a097cbe663130f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'shared': 0,\n",
       " 'decoder.embed_tokens': 0,\n",
       " 'encoder.embed_tokens': 0,\n",
       " 'lm_head': 0,\n",
       " 'encoder.block.0': 0,\n",
       " 'encoder.block.1': 0,\n",
       " 'encoder.block.2': 0,\n",
       " 'encoder.block.3': 0,\n",
       " 'encoder.block.4': 0,\n",
       " 'encoder.block.5': 1,\n",
       " 'encoder.block.6': 1,\n",
       " 'encoder.block.7': 1,\n",
       " 'encoder.block.8': 1,\n",
       " 'encoder.block.9': 1,\n",
       " 'encoder.block.10': 1,\n",
       " 'encoder.block.11': 1,\n",
       " 'encoder.final_layer_norm': 1,\n",
       " 'encoder.dropout': 1,\n",
       " 'decoder.block.0': 1,\n",
       " 'decoder.block.1': 1,\n",
       " 'decoder.block.2': 2,\n",
       " 'decoder.block.3': 2,\n",
       " 'decoder.block.4': 2,\n",
       " 'decoder.block.5': 2,\n",
       " 'decoder.block.6': 2,\n",
       " 'decoder.block.7': 2,\n",
       " 'decoder.block.8': 2,\n",
       " 'decoder.block.9': 3,\n",
       " 'decoder.block.10': 3,\n",
       " 'decoder.block.11': 3,\n",
       " 'decoder.final_layer_norm': 3,\n",
       " 'decoder.dropout': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration, T5Config\n",
    "from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name = \"t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\") # Device_map splits the load over multiple GPUs, this seems to be quite new\n",
    "model.hf_device_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset class supports chaining use of maps (like a monad). We tokenize the raw input strings and split into a train and test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that dataset returns data as lists, you have to [explicitly](https://discuss.huggingface.co/t/dataset-map-return-only-list-instead-torch-tensors/15767/2) set the format to tensors. This function returns nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf214765cf2464b96f21e0e6f6d51df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b611d43040954a2e8d559f65c0d84769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 700\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_wrapper(examples):\n",
    "    return tokenizer(examples[\"in\"], text_target=examples[\"out\"], padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "tokenized_datasets = dataset.map(tokenize_wrapper).remove_columns([\"in\", \"out\"])\n",
    "\n",
    "tokenized_datasets.set_format(\"pt\", columns=[\"input_ids\"], output_all_columns=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle()#.select(range(100))\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle()#.select(range(100))\n",
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Single GPU\n",
    "Note that the model is located across the vram of multiple GPUs, but I'm not sure if the training itself leverages that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir = \"checkpoints/text2sql-t5small-spider\")\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "# Make the Metric\n",
    "# dont forget to pip uninstall once we change to a proper metric, you should probably leave sk learn in\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metric(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Make the Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_test_dataset,\n",
    "    compute_metrics=compute_metric,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjunteng/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1667\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1928\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1931\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/trainer.py:2699\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2698\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2699\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2701\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2702\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/trainer.py:2731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2729\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2730\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2731\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2732\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2733\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2734\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1679\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[39m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1678\u001b[0m     \u001b[39m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m-> 1679\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1680\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1681\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1682\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1683\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1684\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1685\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1686\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1687\u001b[0m     )\n\u001b[1;32m   1688\u001b[0m \u001b[39melif\u001b[39;00m return_dict \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1689\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1690\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   1691\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1692\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1693\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:987\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    984\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mYou have to initialize the model with valid token embeddings\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    985\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens(input_ids)\n\u001b[0;32m--> 987\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m input_shape\n\u001b[1;32m    989\u001b[0m \u001b[39m# required mask seq length can be calculated via length of past\u001b[39;00m\n\u001b[1;32m    990\u001b[0m mask_seq_length \u001b[39m=\u001b[39m past_key_values[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m seq_length \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m seq_length\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerator for Training\n",
    "\n",
    "To work with Accelerator, we have to use the native pyTorch APIs. HuggingFace provides a guide for using [native PyTorch](https://huggingface.co/docs/transformers/training#train-in-native-pytorch), and a reference for usinge these to [accelerate](https://huggingface.co/docs/transformers/accelerate#prepare-to-accelerate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DataLoader\n",
    "# from torch.utils.data import DataLoader\n",
    "# from accelerate import Accelerator\n",
    "\n",
    "# train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "# test_dataloader = DataLoader(small_test_dataset, batch_size=8)\n",
    "\n",
    "# accelerator = Accelerator(project_dir=f\"checkpoints/text2sql-{model_name}-spider-accelerate\")\n",
    "# type(small_train_dataset[\"input_ids\"][0:2])\n",
    "# # Optimizer and Learning Rate shceduler\n",
    "# from torch.optim import AdamW\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "# from transformers import get_scheduler\n",
    "\n",
    "# num_epochs = 3\n",
    "# num_training_steps = num_epochs * len(train_dataloader)\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "# )\n",
    "# train_dataloader, test_dataloader, model, optimizer = accelerator.prepare(\n",
    "#     train_dataloader, test_dataloader, model, optimizer\n",
    "# )\n",
    "# batch.keys()\n",
    "# batch[\"input_ids\"].size()\n",
    "# # Logging maybe try WandB?\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in train_dataloader:\n",
    "#         # batch = {k: v.to(0) for k, v in batch.items()}\n",
    "#         outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         accelerator.backward(loss)\n",
    "\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "\n",
    "# metric = evaluate.load(\"accuracy\") # Idk what is an accuracy metric\n",
    "# model.eval()\n",
    "# for batch in eval_dataloader:\n",
    "#     batch = {k: v.to(0) for k, v in batch.items()}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**batch)\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "# metric.compute() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing\n",
    "\n",
    "This is the example given on the Picard HF website. The expected output is `SELECT COUNT (*) FROM singer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_inputs = [\n",
    "    \"How many singers do we have? | concert_singer | stadium : stadium_id, location, name, capacity, highest, lowest, average | singer : singer_id, name, country, song_name, song_release_year, age, is_male | concert : concert_id, concert_name, theme, stadium_id, year | singer_in_concert : concert_id, singer_id\",\n",
    "]\n",
    "# raw_inputs = map(test_2_preprocessed, train_json[0:50])\n",
    "inputs = tokenizer(list(raw_inputs), padding=True, return_tensors=\"pt\")\n",
    "print(inputs[\"input_ids\"].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.to(0)\n",
    "outputs = model.generate(inputs.input_ids, max_new_tokens=512)\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inputs = tokenizer(training_inputs[0:100], text_target=training_outputs[0:100], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# model_inputs.keys()\n",
    "# # training\n",
    "# inputs = tokenizer(training_inputs[0:100], padding=True, return_tensors=\"pt\")\n",
    "# labels = tokenizer(training_outputs[0:100], padding=True, return_tensors=\"pt\")\n",
    "# outputs = model(input_ids=inputs.input_ids, labels=labels.input_ids)\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
